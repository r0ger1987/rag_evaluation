# GUIDE D'UTILISATION - FICHIER D'Ã‰VALUATION RAG

## ðŸ“‹ OBJECTIF
Ce fichier Excel permet de mener des tests d'Ã©valuation de systÃ¨mes RAG (Retrieval-Augmented Generation) avec diffÃ©rents paramÃ©trages et configurations.

---

## ðŸ“ STRUCTURE DU FICHIER

### Onglet 1 : INSTRUCTIONS
Guide d'utilisation du fichier (cet onglet)

### Onglet 2 : JEU_OR
Questions de test et rÃ©ponses de rÃ©fÃ©rence (gold standard)
- **id** : Identifiant unique de la question
- **question** : Question Ã  poser au systÃ¨me RAG
- **reponse_reference** : RÃ©ponse attendue (ground truth)
- **contexte_attendu** : Documents sources attendus (optionnel)
- **categorie** : CatÃ©gorie de la question (optionnel)
- **difficulte** : Niveau de difficultÃ© (optionnel)

### Onglet 3 : SOURCES
Documents sources utilisÃ©s pour la gÃ©nÃ©ration
- **doc_id** : Identifiant du document
- **source_name** : Nom du fichier source
- **content** : Contenu textuel du document
- **metadata** : MÃ©tadonnÃ©es JSON (optionnel)

### Onglet 4 : SORTIE_EVALUATIONS
RÃ©sultats des Ã©valuations avec mÃ©triques
- Reprend les colonnes de JEU_OR
- **reponse_modele** : RÃ©ponse gÃ©nÃ©rÃ©e par le systÃ¨me
- **contexte_recupere** : Contextes utilisÃ©s pour la gÃ©nÃ©ration
- **MÃ©triques d'Ã©valuation** : faithfulness, answer_relevancy, context_precision, context_recall
- **MÃ©triques de performance** : latency, tokens, coÃ»t estimÃ©
- **ParamÃ¨tres** : model_provider, temperature, etc.

---

## ðŸš€ PROCESSUS D'UTILISATION

### Ã‰tape 1 : PrÃ©paration des donnÃ©es
1. Remplir l'onglet **JEU_OR** avec vos questions de test
2. Ajouter les documents sources dans l'onglet **SOURCES**
3. VÃ©rifier que les IDs sont uniques et cohÃ©rents

### Ã‰tape 2 : Configuration des tests
ParamÃ¨tres Ã  dÃ©finir dans le fichier .env ou le notebook :
- **MODEL_PROVIDER** : `openai`, `anthropic`, `gemini`, `ollama`
- **TEMPERATURE** : 0.0 Ã  1.0 (contrÃ´le la crÃ©ativitÃ©)
- **TOP_K** : Nombre de tokens Ã  considÃ©rer (ex: 40)
- **TOP_P** : ProbabilitÃ© cumulative (ex: 0.9)
- **MAX_TOKENS** : Longueur maximale de rÃ©ponse (ex: 512)
- **BATCH_SIZE** : Taille des lots pour l'Ã©valuation (ex: 16)

### Ã‰tape 3 : ExÃ©cution de l'Ã©valuation
```bash
# Via Jupyter Notebook
jupyter notebook POC_Eval_Ragas_Langfuse.ipynb

# Ou directement via Python
python evaluate.py --input fichier.xlsx --output resultats.xlsx
```

### Ã‰tape 4 : Analyse des rÃ©sultats
Consulter l'onglet **SORTIE_EVALUATIONS** pour :
- Comparer les rÃ©ponses gÃ©nÃ©rÃ©es vs rÃ©fÃ©rences
- Analyser les scores de qualitÃ©
- Identifier les cas d'Ã©chec
- Mesurer les performances (latence, coÃ»ts)

---

## ðŸ“Š MÃ‰TRIQUES D'Ã‰VALUATION

### MÃ©triques de QualitÃ© (scores de 0 Ã  1)

| MÃ©trique | Description | InterprÃ©tation |
|----------|-------------|----------------|
| **Faithfulness** | FidÃ©litÃ© au contexte | Mesure si la rÃ©ponse est ancrÃ©e dans les documents rÃ©cupÃ©rÃ©s |
| **Answer Relevancy** | Pertinence de la rÃ©ponse | Ã‰value si la rÃ©ponse rÃ©pond bien Ã  la question posÃ©e |
| **Context Precision** | PrÃ©cision du contexte | QualitÃ© des documents rÃ©cupÃ©rÃ©s (sont-ils tous pertinents ?) |
| **Context Recall** | Rappel du contexte | ComplÃ©tude des documents rÃ©cupÃ©rÃ©s (a-t-on tout trouvÃ© ?) |
| **Context Entities Recall** | Rappel des entitÃ©s | PrÃ©sence des entitÃ©s importantes dans le contexte |

### MÃ©triques de Performance

| MÃ©trique | Description | UnitÃ© |
|----------|-------------|-------|
| **Latency** | Temps de gÃ©nÃ©ration | Secondes |
| **Tokens Used** | Tokens consommÃ©s | Nombre |
| **Cost Estimate** | CoÃ»t estimÃ© | USD |

---

## âš™ï¸ PARAMÃˆTRES DE TEST RECOMMANDÃ‰S

### Pour tests de prÃ©cision
```
TEMPERATURE = 0.0
TOP_P = 0.1
MAX_TOKENS = 256
```

### Pour tests de crÃ©ativitÃ©
```
TEMPERATURE = 0.7
TOP_P = 0.9
MAX_TOKENS = 512
```

### Pour tests de robustesse
```
TEMPERATURE = 0.3
TOP_P = 0.5
MAX_TOKENS = 384
```

---

## âš ï¸ POINTS D'ATTENTION

### Bonnes pratiques
- âœ… **IDs uniques** : Chaque question doit avoir un ID unique
- âœ… **Sauvegardes** : CrÃ©er une copie avant chaque test
- âœ… **Versioning** : Nommer les fichiers avec date/version
- âœ… **Documentation** : Noter les paramÃ¨tres utilisÃ©s dans les notes

### Erreurs communes Ã  Ã©viter
- âŒ Dupliquer des IDs dans JEU_OR
- âŒ RÃ©fÃ©rences circulaires entre questions
- âŒ CaractÃ¨res spÃ©ciaux non Ã©chappÃ©s dans les questions
- âŒ Documents sources trop longs (> 8000 tokens)

### Limites techniques
- Maximum 1000 questions par batch d'Ã©valuation
- Documents sources limitÃ©s Ã  8192 tokens chacun
- Temps d'Ã©valuation : ~2-5 secondes par question

---

## ðŸ“ EXEMPLES DE CAS D'USAGE

### Test A/B de modÃ¨les
1. MÃªme dataset, diffÃ©rents MODEL_PROVIDER
2. Comparer les scores de faithfulness et relevancy
3. Analyser les diffÃ©rences de latence et coÃ»t

### Optimisation de paramÃ¨tres
1. MÃªme modÃ¨le, varier TEMPERATURE de 0.0 Ã  1.0
2. Observer l'impact sur la crÃ©ativitÃ© vs prÃ©cision
3. Trouver le sweet spot pour votre use case

### Validation de mise Ã  jour
1. Tester avant/aprÃ¨s ajout de nouveaux documents
2. VÃ©rifier l'amÃ©lioration du context_recall
3. S'assurer qu'il n'y a pas de rÃ©gression

---

## ðŸ†˜ SUPPORT ET RESSOURCES

- **Documentation technique** : voir CLAUDE.md dans le projet
- **Notebooks d'exemple** : `POC_Eval_Ragas_Langfuse.ipynb`
- **Configuration** : fichier `.env.template` pour les variables
- **Logs** : vÃ©rifier `data/logs/` pour le debugging

---

## ðŸ“ˆ WORKFLOW TYPE D'Ã‰VALUATION

```mermaid
graph LR
    A[PrÃ©parer Questions] --> B[Ajouter Sources]
    B --> C[Configurer ParamÃ¨tres]
    C --> D[Lancer Ã‰valuation]
    D --> E[Analyser RÃ©sultats]
    E --> F{Satisfaisant?}
    F -->|Non| G[Ajuster ParamÃ¨tres]
    G --> C
    F -->|Oui| H[Documenter & Archiver]
```

---

_Version 1.0 - Guide d'utilisation pour tests d'Ã©valuation RAG_