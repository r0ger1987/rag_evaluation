# ğŸ“Š DOCUMENTATION COMPLÃˆTE DES MÃ‰TRIQUES RAGAS

## ğŸ¯ Vue d'ensemble

Ragas (Retrieval Augmented Generation Assessment) fournit un ensemble de mÃ©triques pour Ã©valuer la qualitÃ© des systÃ¨mes RAG. Ces mÃ©triques Ã©valuent Ã  la fois la **qualitÃ© de rÃ©cupÃ©ration** (retrieval) et la **qualitÃ© de gÃ©nÃ©ration** (generation).

---

## ğŸ“ˆ Les 5 MÃ©triques Essentielles pour l'Ã‰valuation RAG

### 1. ğŸ¯ Faithfulness (FidÃ©litÃ©)

**DÃ©finition officielle** : Mesure Ã  quel point la rÃ©ponse gÃ©nÃ©rÃ©e est **factuellement cohÃ©rente** avec les contextes rÃ©cupÃ©rÃ©s.

**Ce qu'elle Ã©value** : La fidÃ©litÃ© vÃ©rifie que le systÃ¨me ne "hallucine" pas - toutes les affirmations dans la rÃ©ponse doivent Ãªtre supportÃ©es par les documents rÃ©cupÃ©rÃ©s.

**DonnÃ©es requises** :
- `answer` : La rÃ©ponse gÃ©nÃ©rÃ©e par votre systÃ¨me RAG
- `contexts` : Les contextes/documents rÃ©cupÃ©rÃ©s

**Formule de calcul** :
```
Faithfulness = Nombre d'affirmations supportÃ©es par les contextes
              Ã· Nombre total d'affirmations dans la rÃ©ponse
```

**Processus dÃ©taillÃ©** :
1. La rÃ©ponse est dÃ©composÃ©e en affirmations individuelles
2. Chaque affirmation est vÃ©rifiÃ©e contre les contextes
3. Score = ratio des affirmations vÃ©rifiÃ©es

**InterprÃ©tation des scores** :
- **1.0** : Parfait - Toutes les affirmations sont supportÃ©es par les contextes
- **0.8-0.99** : TrÃ¨s bon - Quelques dÃ©tails mineurs non supportÃ©s
- **0.5-0.79** : Moyen - Plusieurs affirmations non vÃ©rifiables
- **< 0.5** : ProblÃ©matique - Beaucoup d'hallucinations

**Exemple** :
```
Question : "Quand a eu lieu le premier Super Bowl ?"
Contexte : "Le premier Super Bowl s'est tenu le 15 janvier 1967"
RÃ©ponse : "Le premier Super Bowl a eu lieu le 15 janvier 1967"
Score : 1.0 âœ… (Totalement fidÃ¨le au contexte)
```

---

### 2. âœ… Answer Correctness (Correction de la RÃ©ponse)

**DÃ©finition officielle** : Ã‰value la **correction factuelle** de la rÃ©ponse gÃ©nÃ©rÃ©e par rapport Ã  une **rÃ©ponse de rÃ©fÃ©rence fournie par les mÃ©tiers**.

**Ce qu'elle Ã©value** : La qualitÃ© de la rÃ©ponse RAG comparÃ©e aux attentes mÃ©tier documentÃ©es.

**DonnÃ©es requises** :
- `answer` : La rÃ©ponse gÃ©nÃ©rÃ©e par votre systÃ¨me RAG
- `reference` : La rÃ©ponse de rÃ©fÃ©rence validÃ©e par les mÃ©tiers

**MÃ©thode de calcul** :
La mÃ©trique combine deux aspects :
1. **SimilaritÃ© sÃ©mantique** : Alignement du sens entre rÃ©ponse et rÃ©fÃ©rence
2. **Correction factuelle** : VÃ©rification des faits et donnÃ©es

**Formule** :
```
Answer Correctness = Î± Ã— SimilaritÃ©_SÃ©mantique + (1-Î±) Ã— Score_Factuel
(oÃ¹ Î± est un poids, gÃ©nÃ©ralement 0.5)
```

**InterprÃ©tation des scores** :
- **0.9-1.0** : Excellent - RÃ©ponse quasi-identique Ã  la rÃ©fÃ©rence mÃ©tier
- **0.7-0.89** : Bon - RÃ©ponse correcte avec variations mineures acceptables
- **0.5-0.69** : Moyen - Quelques erreurs ou omissions importantes
- **< 0.5** : Faible - RÃ©ponse incorrecte ou trÃ¨s incomplÃ¨te

**Exemple** :
```
Question : "Quel est le dÃ©lai de livraison standard ?"
RÃ©fÃ©rence mÃ©tier : "Le dÃ©lai de livraison standard est de 48 Ã  72 heures"
RÃ©ponse RAG : "La livraison prend gÃ©nÃ©ralement 2 Ã  3 jours"
Score : ~0.85 âœ… (Correcte, formulation diffÃ©rente mais Ã©quivalente)

RÃ©ponse RAG : "La livraison prend 1 semaine"
Score : ~0.3 âŒ (Incorrecte, information erronÃ©e)
```

**Cas d'usage** :
- Validation de la conformitÃ© aux processus mÃ©tier
- VÃ©rification de l'exactitude des informations critiques
- Assurance qualitÃ© pour les rÃ©ponses client

---

### 3. ğŸ’¬ Answer Relevancy (Pertinence de la RÃ©ponse)

**âš ï¸ CLARIFICATION IMPORTANTE** : 
Answer Relevancy dans Ragas **NE compare PAS** avec une rÃ©ponse de rÃ©fÃ©rence. Elle Ã©value si la rÃ©ponse gÃ©nÃ©rÃ©e **rÃ©pond bien Ã  la question posÃ©e**.

**DÃ©finition officielle** : Ã‰value Ã  quel point la rÃ©ponse gÃ©nÃ©rÃ©e est **pertinente par rapport Ã  la question**.

**Ce qu'elle Ã©value** : La pertinence vÃ©rifie que la rÃ©ponse reste focalisÃ©e sur ce qui est demandÃ©, sans dÃ©vier du sujet.

**DonnÃ©es requises** :
- `question` : La question posÃ©e
- `answer` : La rÃ©ponse gÃ©nÃ©rÃ©e par votre systÃ¨me RAG

**MÃ©thode de calcul** :
1. Le LLM gÃ©nÃ¨re des questions potentielles Ã  partir de la rÃ©ponse
2. Ces questions gÃ©nÃ©rÃ©es sont comparÃ©es Ã  la question originale
3. Score = similaritÃ© moyenne entre questions gÃ©nÃ©rÃ©es et question originale

**InterprÃ©tation des scores** :
- **0.9-1.0** : Excellent - La rÃ©ponse rÃ©pond directement et prÃ©cisÃ©ment
- **0.7-0.89** : Bon - La rÃ©ponse est pertinente avec quelques digressions
- **0.5-0.69** : Moyen - La rÃ©ponse contient des informations hors-sujet
- **< 0.5** : Faible - La rÃ©ponse ne rÃ©pond pas vraiment Ã  la question

**Exemple** :
```
Question : "Quelle est la capitale de la France ?"
RÃ©ponse : "La capitale de la France est Paris"
Score : ~1.0 âœ… (RÃ©ponse directe et pertinente)

Question : "Quelle est la capitale de la France ?"
RÃ©ponse : "La France est un pays d'Europe avec beaucoup de villes"
Score : ~0.3 âŒ (RÃ©ponse vague, ne rÃ©pond pas directement)
```

---

### 4. ğŸ¯ Context Precision (PrÃ©cision du Contexte)

**DÃ©finition officielle** : Mesure si les contextes pertinents sont **bien classÃ©s** (apparaissent en premier).

**Ce qu'elle Ã©value** : La qualitÃ© du ranking - les chunks les plus pertinents doivent Ãªtre en haut de la liste.

**DonnÃ©es requises** :
- `question` : La question posÃ©e
- `contexts` : Liste ordonnÃ©e des contextes rÃ©cupÃ©rÃ©s
- `ground_truths` : Contextes de rÃ©fÃ©rence attendus

**Formule de calcul** :
```
Context Precision = Moyenne(Precision@k pour chaque contexte pertinent)
oÃ¹ Precision@k = Nombre de contextes pertinents dans les k premiers / k
```

**Processus** :
1. Pour chaque position k, calculer la prÃ©cision
2. PondÃ©rer selon la position (les premiers ont plus de poids)
3. Moyenner les scores

**InterprÃ©tation des scores** :
- **0.9-1.0** : Excellent ranking - Contextes pertinents en tÃªte
- **0.7-0.89** : Bon - Quelques contextes mal classÃ©s
- **0.5-0.69** : Moyen - Ordre sous-optimal
- **< 0.5** : Faible - Contextes pertinents noyÃ©s dans le bruit

**Exemple** :
```
Contextes rÃ©cupÃ©rÃ©s : [TrÃ¨s pertinent, TrÃ¨s pertinent, Peu pertinent, Non pertinent]
Score : ~0.83 âœ… (Les 2 contextes pertinents sont en tÃªte)

Contextes rÃ©cupÃ©rÃ©s : [Non pertinent, Peu pertinent, TrÃ¨s pertinent, TrÃ¨s pertinent]
Score : ~0.25 âŒ (Les contextes pertinents sont mal classÃ©s)
```

---

### 5. ğŸ“š Context Recall (Rappel du Contexte)

**DÃ©finition officielle** : Mesure **combien d'informations pertinentes** ont Ã©tÃ© rÃ©cupÃ©rÃ©es par rapport aux documents de rÃ©fÃ©rence.

**Ce qu'elle Ã©value** : La complÃ©tude de la rÃ©cupÃ©ration - s'assure qu'aucune information importante n'est manquÃ©e.

**DonnÃ©es requises** :
- `contexts` : Contextes rÃ©cupÃ©rÃ©s par votre systÃ¨me
- `ground_truths` : Contextes de rÃ©fÃ©rence attendus (ou `reference` comme proxy)

**Formule de calcul (version LLM)** :
```
Context Recall = Nombre d'affirmations de la rÃ©fÃ©rence prÃ©sentes dans les contextes
                Ã· Nombre total d'affirmations dans la rÃ©fÃ©rence
```

**Formule de calcul (version non-LLM)** :
```
Context Recall = Nombre de contextes de rÃ©fÃ©rence rÃ©cupÃ©rÃ©s
                Ã· Nombre total de contextes de rÃ©fÃ©rence
```

**InterprÃ©tation des scores** :
- **0.9-1.0** : Excellent - Presque toutes les infos pertinentes rÃ©cupÃ©rÃ©es
- **0.7-0.89** : Bon - La majoritÃ© des infos importantes prÃ©sentes
- **0.5-0.69** : Moyen - Des informations importantes manquent
- **< 0.5** : Faible - Beaucoup d'informations cruciales absentes

**Exemple** :
```
RÃ©fÃ©rence : "Paris est la capitale. Population 2.2M. Tour Eiffel construite en 1889."
Contextes rÃ©cupÃ©rÃ©s : "Paris capitale de France. Tour Eiffel 1889."
Score : ~0.67 (2 infos sur 3 rÃ©cupÃ©rÃ©es)
```

---

## ğŸ”„ Relations entre les MÃ©triques

```mermaid
graph TD
    A[Question] --> B[RÃ©cupÃ©ration]
    B --> C[Contextes]
    C --> D[GÃ©nÃ©ration]
    D --> E[RÃ©ponse]
    
    C -.-> F[Context Precision<br/>Contextes bien classÃ©s ?]
    C -.-> G[Context Recall<br/>Tous les contextes trouvÃ©s ?]
    E -.-> H[Faithfulness<br/>RÃ©ponse fidÃ¨le aux contextes ?]
    E -.-> I[Answer Relevancy<br/>RÃ©ponse pertinente Ã  la question ?]
```

---

## âš ï¸ Points d'Attention Importants

### 1. **Answer Relevancy â‰  Answer Correctness**
- `Answer Relevancy` : La rÃ©ponse rÃ©pond-elle Ã  la question ?
- `Answer Correctness` : La rÃ©ponse est-elle correcte par rapport Ã  une rÃ©fÃ©rence ?
- Pour comparer avec une rÃ©ponse de rÃ©fÃ©rence mÃ©tier, utilisez `answer_correctness` (mÃ©trique sÃ©parÃ©e)

### 2. **Ground Truth pour les Contextes**
- `ground_truths` doit Ãªtre une **liste de contextes**, pas juste un nom de document
- Format : Liste de chunks/passages qui devraient Ãªtre rÃ©cupÃ©rÃ©s
- UtilisÃ© pour `context_precision` et `context_recall`

### 3. **DÃ©pendances des MÃ©triques**
| MÃ©trique | Question | Answer | Reference | Contexts | Ground_truths |
|----------|----------|---------|-----------|----------|---------------|
| Faithfulness | âŒ | âœ… | âŒ | âœ… | âŒ |
| Answer Correctness | âŒ | âœ… | âœ… | âŒ | âŒ |
| Answer Relevancy | âœ… | âœ… | âŒ | âŒ | âŒ |
| Context Precision | âœ… | âŒ | âŒ | âœ… | âœ… |
| Context Recall | âŒ | âŒ | âŒ | âœ… | âœ… |

---

## ğŸ“Š StratÃ©gies d'AmÃ©lioration

### Si Answer Correctness est faible (< 0.7)
**ProblÃ¨me** : Les rÃ©ponses ne correspondent pas aux attentes mÃ©tier
**Solutions** :
- AmÃ©liorer la qualitÃ© des contextes rÃ©cupÃ©rÃ©s
- Enrichir la base documentaire avec plus de dÃ©tails
- Ajuster le prompt pour mieux utiliser les contextes
- VÃ©rifier que les documents de rÃ©fÃ©rence sont Ã  jour

### Si Faithfulness est faible (< 0.7)
**ProblÃ¨me** : Le modÃ¨le hallucine, invente des informations
**Solutions** :
- Ajuster le prompt pour Ãªtre plus strict sur l'utilisation des sources
- RÃ©duire la tempÃ©rature du modÃ¨le
- ImplÃ©menter des garde-fous contre les hallucinations

### Si Answer Relevancy est faible (< 0.7)
**ProblÃ¨me** : Les rÃ©ponses dÃ©vient du sujet
**Solutions** :
- AmÃ©liorer le prompt pour rester focalisÃ©
- ImplÃ©menter un re-ranking des rÃ©ponses
- Filtrer les informations non pertinentes

### Si Context Precision est faible (< 0.7)
**ProblÃ¨me** : Les meilleurs contextes ne sont pas en premier
**Solutions** :
- AmÃ©liorer l'algorithme de ranking (BM25, reranking models)
- Optimiser les embeddings
- Ajuster les seuils de similaritÃ©

### Si Context Recall est faible (< 0.7)
**ProblÃ¨me** : Des documents importants ne sont pas rÃ©cupÃ©rÃ©s
**Solutions** :
- Augmenter le nombre de documents rÃ©cupÃ©rÃ©s (top_k)
- AmÃ©liorer la stratÃ©gie de chunking
- Enrichir les mÃ©tadonnÃ©es des documents

---

## ğŸš€ Utilisation dans le Code

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    answer_correctness,  # Pour comparer avec rÃ©fÃ©rence mÃ©tier
    context_precision,
    context_recall
)
from datasets import Dataset

# PrÃ©parer les donnÃ©es
dataset = Dataset.from_dict({
    "question": questions,           # Liste des questions
    "answer": answers,               # RÃ©ponses gÃ©nÃ©rÃ©es par votre RAG
    "reference": references,         # RÃ©ponses de rÃ©fÃ©rence mÃ©tier
    "contexts": contexts,            # Liste de listes de contextes rÃ©cupÃ©rÃ©s
    "ground_truths": ground_truths  # Liste de listes de contextes de rÃ©fÃ©rence
})

# Ã‰valuer
results = evaluate(
    dataset,
    metrics=[
        faithfulness,       # FidÃ©litÃ© aux sources
        answer_correctness, # Correction vs rÃ©fÃ©rence mÃ©tier
        answer_relevancy,   # Pertinence Ã  la question
        context_precision,  # QualitÃ© du ranking
        context_recall      # ComplÃ©tude de la rÃ©cupÃ©ration
    ]
)

print(results)  # Scores pour chaque mÃ©trique
```

---

## ğŸ“š RÃ©fÃ©rences

- [Documentation officielle Ragas](https://docs.ragas.io/)
- [Ragas GitHub](https://github.com/explodinggradients/ragas)
- [Article original](https://arxiv.org/abs/2309.15217)

---

*Cette documentation est basÃ©e sur Ragas v0.2+ et la documentation officielle.*