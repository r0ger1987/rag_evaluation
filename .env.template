# ================================
# LANGFUSE CONFIGURATION
# ================================
# Langfuse API credentials for trace collection
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key_here
LANGFUSE_SECRET_KEY=your_langfuse_secret_key_here
LANGFUSE_BASE_URL=https://cloud.langfuse.com

# ================================
# FILE CONFIGURATION
# ================================
# Paths to input/output files
REFERENCE_CSV=./data/reference/reference_qa.csv
OUTPUT_CSV=./data/results/smartrag_evaluation_results.csv
OUTPUT_JSON=./data/results/smartrag_evaluation_results_detailed.json

# ================================
# SMARTRAG CONFIGURATION
# ================================
# SmartRAG project name in Langfuse (leave empty for all traces)
SMARTRAG_PROJECT_NAME=

# Number of days to look back for traces
EVALUATION_TIMERANGE=7

# ================================
# EVALUATION CONFIGURATION
# ================================
# Minimum confidence score for trace matching (0.0 to 1.0)
MIN_CONFIDENCE_SCORE=0.7

# Include failed traces in evaluation
INCLUDE_FAILED_TRACES=false

# ================================
# LLM CONFIGURATION FOR RAGAS
# ================================
# Provider selection: openai | gemini | claude | ollama
RAGAS_LLM_PROVIDER=openai

# Model name based on provider:
# - OpenAI: gpt-4.1-mini, gpt-4o-mini, gpt-4, gpt-4-turbo
# - Gemini: gemini-2.5-flash, gemini-2.5-flash-lite, gemini-2.5-pro
# - Claude: claude-3-5-haiku-20241022, claude-3-5-sonnet-20241022, claude-3-5-opus-latest
# - Ollama: llama2, mistral, qwen3:8b, deepseek-r1:8b, etc.
RAGAS_MODEL_NAME=gpt-4.1-mini

# ================================
# API KEYS
# ================================
# OpenAI API Key (required if RAGAS_LLM_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key_here

# Google API Key (required if RAGAS_LLM_PROVIDER=gemini)
GOOGLE_API_KEY=your_google_api_key_here

# Anthropic API Key (required if RAGAS_LLM_PROVIDER=claude)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ================================
# OLLAMA CONFIGURATION
# ================================
# Configuration for local Ollama models (if RAGAS_LLM_PROVIDER=ollama)
RAGAS_OLLAMA_BASE_URL=http://localhost:11434
RAGAS_OLLAMA_MODEL=llama2

# ================================
# ADVANCED CONFIGURATION
# ================================
# Batch size for evaluation (affects memory usage)
EVALUATION_BATCH_SIZE=4

# Enable debug logging
DEBUG_MODE=false

# Export intermediate results
EXPORT_INTERMEDIATE_RESULTS=false