# INSTRUCTIONS MANUELLES - Ã‰VALUATION RAG

## ðŸ“‹ Vue d'ensemble

Ce guide explique comment utiliser le systÃ¨me d'Ã©valuation RAG manuel basÃ© sur les fichiers existants du projet. L'Ã©valuation utilise le fichier CSV de rÃ©fÃ©rence et le notebook Jupyter dÃ©diÃ© pour Ã©valuer les performances de votre systÃ¨me RAG avec les mÃ©triques Ragas.

---

## ðŸ—‚ï¸ Fichiers disponibles

### Fichier de donnÃ©es de rÃ©fÃ©rence
- **Fichier** : `data/reference/reference_qa_manuel_template.csv`
- **Description** : Template avec 10 questions d'exemple du Projet NÃ©o
- **Contenu** : Questions sur management, technique et projet avec rÃ©ponses de rÃ©fÃ©rence

### Notebook d'Ã©valuation
- **Fichier** : `notebooks/SmartRAG_Tutorial_Manuel.ipynb`
- **Description** : Notebook complet pour l'Ã©valuation manuelle avec Ragas
- **FonctionnalitÃ©s** : Configuration, chargement, Ã©valuation et analyse

---

## ðŸ“ Structure du fichier CSV

Le fichier `reference_qa_manuel_template.csv` contient **8 colonnes** pour le POC SharePoint :

### Colonnes d'input (Ã  remplir par les mÃ©tiers)
- `question_id` : Identifiant unique (QSP001, QSP002...)
- `question` : Question Ã  poser au systÃ¨me RAG
- `reference_answer` : RÃ©ponse de rÃ©fÃ©rence attendue
- `sharepoint_document` : **Nom exact du document SharePoint de rÃ©fÃ©rence**

### Colonnes pour l'Ã©valuation Ragas
- `ragas_question` : Question formatÃ©e pour Ragas (copie de `question`)
- `ragas_answer` : **RÃ©ponse gÃ©nÃ©rÃ©e par votre systÃ¨me RAG** (Ã  complÃ©ter)
- `ragas_contexts` : **Contextes rÃ©cupÃ©rÃ©s par votre systÃ¨me** (Ã  complÃ©ter)
- `ragas_ground_truth` : **Nom du document SharePoint** (copie de `sharepoint_document`)

### Exemple de donnÃ©es
```csv
question_id,question,reference_answer,sharepoint_document,ragas_question,ragas_answer,ragas_contexts,ragas_ground_truth
QSP001,"Quelle est la procÃ©dure pour demander un congÃ© ?","La demande de congÃ© doit Ãªtre soumise via l'application RH au moins 15 jours avant...","RH_Procedures_Conges_2025.docx","Quelle est la procÃ©dure pour demander un congÃ© ?","","","RH_Procedures_Conges_2025.docx"
```

### âš ï¸ Important pour le POC
- **`sharepoint_document`** : Les mÃ©tiers doivent indiquer le nom exact du fichier SharePoint contenant la rÃ©ponse
- **`ragas_ground_truth`** : UtilisÃ© par Ragas pour vÃ©rifier que le bon document a Ã©tÃ© rÃ©cupÃ©rÃ©

---

## âš™ï¸ Utilisation du notebook d'Ã©valuation

### Ã‰tape 1 : Lancement du notebook
```bash
# DÃ©marrer Jupyter
cd /home/roger/RAG/ragas
jupyter notebook notebooks/SmartRAG_Tutorial_Manuel.ipynb

# Ou avec Jupyter Lab  
jupyter lab notebooks/SmartRAG_Tutorial_Manuel.ipynb
```

### Ã‰tape 2 : Configuration des modÃ¨les
Le notebook supporte **4 providers LLM** ==> A adapter avec les modÃ¨les FM Aws Bedrock

#### OpenAI (recommandÃ©)
```python
RAGAS_LLM_PROVIDER = "openai"
RAGAS_MODEL_NAME = "gpt-4.1-mini"        
OPENAI_API_KEY = "your_openai_key_here"
```

#### Google Gemini
```python
RAGAS_LLM_PROVIDER = "gemini"
RAGAS_MODEL_NAME = "gemini-2.5-flash"    # Avec capacitÃ©s de raisonnement
GOOGLE_API_KEY = "your_google_key_here"
```

#### Anthropic Claude
```python
RAGAS_LLM_PROVIDER = "claude"
RAGAS_MODEL_NAME = "claude-3-5-haiku-20241022"  # Plus rapide que Claude 3 Opus
ANTHROPIC_API_KEY = "your_anthropic_key_here"
```

#### Ollama (local)
```python
RAGAS_LLM_PROVIDER = "ollama"
RAGAS_MODEL_NAME = "llama3.2"            # Ou mistral, qwen3, etc.
OLLAMA_BASE_URL = "http://localhost:11434"
```

### Ã‰tape 3 : Chargement des donnÃ©es
Le notebook charge automatiquement `reference_qa_manuel_template.csv` et valide :
- âœ… **10 questions d'exemple** du POC SharePoint (RH, IT, SÃ©curitÃ©)
- âœ… **Documents SharePoint** : Noms de fichiers .docx et .pdf
- âœ… **DonnÃ©es Ragas** : question, answer (vide), contexts (vide), ground_truth (document SharePoint)

---

## ðŸ“Š Les 5 MÃ©triques Ragas UtilisÃ©es (Documentation Officielle)

### 1. ðŸŽ¯ Faithfulness (FidÃ©litÃ©) - Score 0-1
**DÃ©finition** : Mesure si la rÃ©ponse est **factuellement cohÃ©rente** avec les contextes rÃ©cupÃ©rÃ©s
**Formule** : `Nombre d'affirmations supportÃ©es / Total des affirmations dans la rÃ©ponse`
**DonnÃ©es utilisÃ©es** : `ragas_contexts` + `ragas_answer`
**Objectif** : DÃ©tecter les hallucinations (informations inventÃ©es)

**Exemple** :
- Question : "Qui est le manager de Sophie Martin ?"
- Contexte : "Marc Dubois manage Sophie Martin"
- RÃ©ponse : "Marc Dubois est le manager. Il travaille depuis 10 ans."
- Score : 0.5 (1 affirmation vÃ©rifiable sur 2 - "10 ans" est inventÃ©)

### 2. âœ… Answer Correctness (Correction) - Score 0-1
**DÃ©finition** : Ã‰value si la rÃ©ponse est **correcte par rapport Ã  la rÃ©fÃ©rence mÃ©tier**
**Formule** : `Î± Ã— SimilaritÃ©_SÃ©mantique + (1-Î±) Ã— Score_Factuel`
**DonnÃ©es utilisÃ©es** : `ragas_answer` + `reference_answer`
**Objectif** : Valider que la rÃ©ponse correspond aux attentes mÃ©tier documentÃ©es

**Exemple** :
- RÃ©fÃ©rence : "Le dÃ©lai est de 48-72 heures"
- RÃ©ponse RAG : "2 Ã  3 jours" â†’ Score ~0.85 âœ…
- RÃ©ponse RAG : "1 semaine" â†’ Score ~0.3 âŒ

### 3. ðŸ’¬ Answer Relevancy (Pertinence) - Score 0-1  
**DÃ©finition** : Ã‰value si la rÃ©ponse **rÃ©pond directement Ã  la question** (sans dÃ©vier)
**âš ï¸ DIFFÃ‰RENT** de Answer Correctness : vÃ©rifie la pertinence, pas la correction
**Formule** : `SimilaritÃ© entre question originale et questions gÃ©nÃ©rÃ©es depuis la rÃ©ponse`
**DonnÃ©es utilisÃ©es** : `ragas_question` + `ragas_answer`

### 4. ðŸŽ¯ Context Precision (PrÃ©cision) - Score 0-1
**DÃ©finition** : VÃ©rifie si les **contextes pertinents sont bien classÃ©s** en tÃªte de liste
**Formule** : `Moyenne(Precision@k)` pour chaque contexte pertinent
**DonnÃ©es utilisÃ©es** : `ragas_question` + `ragas_contexts` + `ragas_ground_truth`
**Objectif** : Ã‰valuer la qualitÃ© du ranking des documents

### 5. ðŸ“š Context Recall (Rappel) - Score 0-1
**DÃ©finition** : Mesure si **tous les contextes importants** ont Ã©tÃ© rÃ©cupÃ©rÃ©s
**Formule** : `Infos de rÃ©fÃ©rence trouvÃ©es / Total des infos de rÃ©fÃ©rence`
**DonnÃ©es utilisÃ©es** : `ragas_contexts` + `ragas_ground_truth`
**Pour SharePoint** : VÃ©rifie que les bons documents SharePoint sont dans les contextes rÃ©cupÃ©rÃ©s

---

## ðŸš€ Workflow d'Ã©valuation

### 1. PrÃ©paration
- **Les mÃ©tiers** remplissent les colonnes d'input :
  - `question_id`, `question`, `reference_answer`, `sharepoint_document`
- **Vous** complÃ©tez les colonnes Ragas :
  - `ragas_answer` : RÃ©ponses gÃ©nÃ©rÃ©es par **votre systÃ¨me RAG**
  - `ragas_contexts` : Contextes rÃ©cupÃ©rÃ©s par **votre systÃ¨me RAG** (incluant noms de fichiers SharePoint)

### 2. Ã‰valuation
Le notebook exÃ©cute automatiquement :
```python
# MÃ©triques Ã©valuÃ©es
metrics = [faithfulness, answer_relevancy, context_precision, context_recall]

# Ã‰valuation Ragas
results = evaluate(dataset, metrics=metrics, llm=llm_model)
```

### 3. Analyse
Le notebook gÃ©nÃ¨re :
- **Statistiques globales** par mÃ©trique
- **Analyse par type de document** (.docx, .pdf)
- **Documents SharePoint manquÃ©s** (Context Recall faible)
- **Questions problÃ©matiques** avec scores faibles
- **Visualisations** (graphiques, heatmaps)

### 4. Export
Sauvegarde automatique :
- **CSV dÃ©taillÃ©** : `manual_evaluation_results_detailed.csv`
- **JSON complet** : `manual_evaluation_results.json`
- **Graphiques** : fichiers PNG dans `/results/`

---

## ðŸ“ˆ InterprÃ©tation des rÃ©sultats

### Scores cibles recommandÃ©s
| MÃ©trique | Excellent | Bon | Ã€ amÃ©liorer | Critique |
|----------|-----------|-----|-------------|----------|
| Faithfulness | > 0.9 | 0.7-0.9 | 0.5-0.7 | < 0.5 |
| Answer Relevancy | > 0.8 | 0.6-0.8 | 0.4-0.6 | < 0.4 |
| Context Precision | > 0.8 | 0.6-0.8 | 0.4-0.6 | < 0.4 |
| Context Recall | > 0.9 | 0.7-0.9 | 0.5-0.7 | < 0.5 |

### Actions d'amÃ©lioration pour SharePoint
| Score faible | Cause probable | Action recommandÃ©e |
|--------------|----------------|-------------------|
| Faithfulness | Hallucinations | AmÃ©liorer le prompt, restreindre la gÃ©nÃ©ration |
| Answer Relevancy | RÃ©ponse hors-sujet | Optimiser la comprÃ©hension de la question |
| Context Precision | Mauvais ranking | AmÃ©liorer le reranking des documents SharePoint |
| Context Recall | Document SharePoint manquÃ© | VÃ©rifier indexation SharePoint, ajuster les mÃ©tadonnÃ©es |

---

## âš ï¸ Bonnes pratiques

### QualitÃ© des donnÃ©es pour SharePoint
- âœ… VÃ©rifier que `sharepoint_document` correspond au nom exact du fichier sur SharePoint
- âœ… S'assurer que `ragas_contexts` contient les noms de documents SharePoint rÃ©cupÃ©rÃ©s
- âœ… Valider que `ragas_ground_truth` correspond bien au document SharePoint de rÃ©fÃ©rence

### Configuration stable
- âœ… Fixer les seeds pour la reproductibilitÃ©
- âœ… Utiliser les mÃªmes versions de dÃ©pendances
- âœ… Documenter la configuration utilisÃ©e

### Validation manuelle
- âœ… Examiner quelques rÃ©ponses manuellement
- âœ… VÃ©rifier la cohÃ©rence des scores Ragas
- âœ… Comparer avec d'autres mÃ©triques si disponibles