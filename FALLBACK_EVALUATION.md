# ğŸš¨ Ã‰valuation de Fallback - Version Stable

Si vous continuez Ã  avoir des problÃ¨mes avec l'Ã©valuation complÃ¨te, voici une version simplifiÃ©e qui fonctionne de maniÃ¨re plus stable.

## ğŸ“ Code de Fallback pour Jupyter

Copiez-collez ce code dans une nouvelle cellule si l'Ã©valuation principale Ã©choue :

```python
# ğŸ›Ÿ Ã‰VALUATION SIMPLIFIÃ‰E - VERSION FALLBACK

import pandas as pd
from ragas.metrics import faithfulness, answer_correctness
from ragas import evaluate

print("ğŸ›Ÿ Ã‰valuation simplifiÃ©e avec mÃ©triques stables...")

# Utiliser seulement les mÃ©triques les plus stables
stable_metrics = [
    faithfulness,      # TrÃ¨s stable, nÃ©cessite seulement LLM
    answer_correctness # Stable, compare answer et reference
]

try:
    # Configuration minimale
    for metric in stable_metrics:
        if hasattr(metric, 'llm'):
            metric.llm = ragas_llm
    
    print("âœ… MÃ©triques configurÃ©es")
    
    # Ã‰valuation avec dataset rÃ©duit si nÃ©cessaire
    small_dataset = dataset.select(range(min(5, len(dataset))))  # Tester sur 5 premiÃ¨res questions
    
    results = evaluate(
        dataset=small_dataset,
        metrics=stable_metrics,
        llm=ragas_llm,
        raise_exceptions=False
    )
    
    print("\nâœ… Ã‰VALUATION FALLBACK TERMINÃ‰E !")
    
    # Affichage des rÃ©sultats
    if hasattr(results, 'scores'):
        print("\nğŸ“Š SCORES (5 premiÃ¨res questions) :")
        for metric, score in results.scores.items():
            if score is not None:
                emoji = 'ğŸŸ¢' if score >= 0.7 else 'ğŸŸ¡' if score >= 0.5 else 'ğŸ”´'
                print(f"   {emoji} {metric}: {score:.3f}")
    
    print("\nğŸ’¡ Si cette version fonctionne, le problÃ¨me vient des embeddings ou des mÃ©triques de contexte.")
    
except Exception as e:
    print(f"âŒ MÃªme l'Ã©valuation fallback Ã©choue : {e}")
    print("\nğŸ”§ Solutions alternatives :")
    print("1. VÃ©rifiez que votre clÃ© API OpenAI est valide")
    print("2. Essayez avec un autre provider (Gemini, Claude)")
    print("3. Testez avec Ollama en local")
```

## ğŸ”§ MÃ©triques par ordre de stabilitÃ©

### âœ… TrÃ¨s stables (nÃ©cessitent seulement LLM)
- **Faithfulness** : DÃ©tecte les hallucinations
- **Answer Correctness** : Compare avec rÃ©fÃ©rence

### ğŸŸ¡ ModÃ©rÃ©ment stables (nÃ©cessitent LLM + embeddings)
- **Answer Relevancy** : Peut avoir des problÃ¨mes d'embeddings
- **Context Precision** : DÃ©pend de la qualitÃ© des ground_truths
- **Context Recall** : DÃ©pend des embeddings pour les comparaisons

## ğŸ¯ Test Minimal

Si mÃªme le fallback ne fonctionne pas, testez cette version ultra-minimale :

```python
# Test ultra-minimal
from ragas.metrics import faithfulness

# Test sur 1 seule question
single_item = {
    'question': [dataset[0]['question']],
    'answer': [dataset[0]['answer']], 
    'contexts': [dataset[0]['contexts']]
}

single_dataset = Dataset.from_dict(single_item)

try:
    result = evaluate(
        dataset=single_dataset,
        metrics=[faithfulness],
        llm=ragas_llm
    )
    print("âœ… Test minimal rÃ©ussi !")
    print(f"Faithfulness: {result.scores['faithfulness']:.3f}")
except Exception as e:
    print(f"âŒ Test minimal Ã©chouÃ© : {e}")
```

## ğŸ”„ Solutions Alternatives

### Option 1: Changer de Provider
```python
# Essayer avec Gemini (souvent plus stable)
RAGAS_LLM_PROVIDER = 'gemini'
GOOGLE_API_KEY = 'your-key'
```

### Option 2: Ollama en Local
```python
# Plus lent mais trÃ¨s stable
RAGAS_LLM_PROVIDER = 'ollama'
OLLAMA_MODEL = 'llama3.2'
```

### Option 3: Ã‰valuation Manuel
```python
# Calculer les mÃ©triques une par une
for i in range(len(dataset)):
    item = dataset[i]
    print(f"\nQuestion {i+1}: {item['question'][:50]}...")
    
    # Test faithfulness sur cet item
    try:
        single_result = evaluate(
            dataset=Dataset.from_dict({k: [v] for k, v in item.items()}),
            metrics=[faithfulness],
            llm=ragas_llm
        )
        print(f"âœ… Faithfulness: {single_result.scores['faithfulness']:.3f}")
    except Exception as e:
        print(f"âŒ Erreur: {e}")
```