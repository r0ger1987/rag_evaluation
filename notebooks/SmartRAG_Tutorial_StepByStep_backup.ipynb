{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# üéØ Tutorial Pas √† Pas : √âvaluation SmartRAG avec Ragas\n",
    "\n",
    "Ce notebook vous guide **√©tape par √©tape** pour √©valuer votre syst√®me SmartRAG en utilisant Ragas via Langfuse.\n",
    "\n",
    "## üìã Pr√©requis\n",
    "- SmartRAG configur√© et connect√© √† Langfuse\n",
    "- Cl√©s API Langfuse\n",
    "- Une cl√© API pour l'√©valuation (OpenAI/Gemini/Claude)\n",
    "- Questions de test pr√©par√©es\n",
    "\n",
    "## üóÇÔ∏è Ce que vous allez apprendre\n",
    "1. **Configurer** l'√©valuation avec vos cl√©s API\n",
    "2. **Pr√©parer** vos questions de r√©f√©rence  \n",
    "3. **R√©cup√©rer** les traces SmartRAG depuis Langfuse\n",
    "4. **√âvaluer** la qualit√© avec les m√©triques Ragas\n",
    "5. **Analyser** les r√©sultats et identifier les am√©liorations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-config",
   "metadata": {},
   "source": [
    "## üîß √âtape 1 : Configuration initiale\n",
    "\n",
    "### 1.1 Installation des d√©pendances\n",
    "\n",
    "Ex√©cutez cette cellule pour installer tous les packages n√©cessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/roger/RAG/ragas/venv/lib/python3.12/site-packages (25.2)\n",
      "‚úÖ Installation termin√©e !\n"
     ]
    }
   ],
   "source": [
    "# Installation des d√©pendances (mod√®les 2025)\n",
    "!pip install --upgrade pip\n",
    "!pip install langfuse>=3.0.0 ragas>=0.3.0 pandas matplotlib seaborn python-dotenv\n",
    "!pip install openai>=1.0.0 google-generativeai>=0.8.0 anthropic>=0.64.0\n",
    "!pip install datasets>=4.0.0 tqdm>=4.65.0\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-2",
   "metadata": {},
   "source": [
    "### 1.2 Configuration de vos cl√©s API\n",
    "\n",
    "**üìù Action requise :** Compl√©tez les variables ci-dessous avec vos vraies cl√©s API :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config-keys",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Configuration choisie:\n",
      "   Provider: openai\n",
      "   Mod√®le: gpt-4.1-mini\n",
      "   Projet SmartRAG: Tous\n",
      "   P√©riode: 7 derniers jours\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# üîë LANGFUSE - Remplacez par vos vraies cl√©s\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-b65665b3-76ec-4a73-95ac-91254ae9af8a\"  # üëà Votre cl√© publique Langfuse\n",
    "LANGFUSE_SECRET_KEY = \"your-openai-api-key\"  # üëà Votre cl√© secr√®te Langfuse  \n",
    "LANGFUSE_BASE_URL = \"https://cloud.langfuse.com\"    # Ou votre instance priv√©e\n",
    "\n",
    "# ü§ñ LLM pour Ragas - Choisissez UN provider et sa cl√©\n",
    "RAGAS_LLM_PROVIDER = \"openai\"  # openai | gemini | claude\n",
    "\n",
    "# Cl√©s API pour l'√©valuation (uncommentez celle que vous utilisez)\n",
    "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY_HERE\"      # Pour OpenAI GPT-4.1-mini\n",
    "# GOOGLE_API_KEY = \"AIzaSyAP0wVrreVe1xOx4-3S0axSSWBV6uOsCLI\"       # Pour Gemini 2.5 Flash \n",
    "# ANTHROPIC_API_KEY = \"your-openai-api-key\"    # Pour Claude 3.5 Haiku\n",
    "\n",
    "# üéØ SmartRAG - Param√®tres d'√©valuation\n",
    "SMARTRAG_PROJECT_NAME = \"\"  # Nom de votre projet SmartRAG (ou laissez vide pour tous)\n",
    "EVALUATION_TIMERANGE = 7   # Nombre de jours √† analyser\n",
    "\n",
    "# Mod√®les 2025 recommand√©s\n",
    "MODEL_MAP = {\n",
    "    \"openai\": \"gpt-4.1-mini\",          # Nouveau mod√®le 2025, -83% co√ªt\n",
    "    \"gemini\": \"gemini-2.5-flash\",      # Derni√®re version avec thinking\n",
    "    \"claude\": \"claude-3-5-haiku-20241022\"  # Version 2024 la plus r√©cente\n",
    "}\n",
    "\n",
    "RAGAS_MODEL_NAME = MODEL_MAP[RAGAS_LLM_PROVIDER]\n",
    "\n",
    "print(f\"üéØ Configuration choisie:\")\n",
    "print(f\"   Provider: {RAGAS_LLM_PROVIDER}\")\n",
    "print(f\"   Mod√®le: {RAGAS_MODEL_NAME}\")\n",
    "print(f\"   Projet SmartRAG: {SMARTRAG_PROJECT_NAME or 'Tous'}\")\n",
    "print(f\"   P√©riode: {EVALUATION_TIMERANGE} derniers jours\")\n",
    "\n",
    "# Validation des cl√©s\n",
    "if not LANGFUSE_PUBLIC_KEY.startswith(\"pk-lf-\"):\n",
    "    print(\"‚ö†Ô∏è  ATTENTION: Remplacez LANGFUSE_PUBLIC_KEY par votre vraie cl√©\")\n",
    "    \n",
    "if RAGAS_LLM_PROVIDER == \"openai\" and not OPENAI_API_KEY.startswith(\"your-openai-api-key\"):\n",
    "    print(\"‚ö†Ô∏è  ATTENTION: Remplacez OPENAI_API_KEY par votre vraie cl√© OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-reference",
   "metadata": {},
   "source": [
    "## üìù √âtape 2 : Pr√©paration des questions de r√©f√©rence\n",
    "\n",
    "### 2.1 Cr√©er votre jeu de test\n",
    "\n",
    "Nous allons cr√©er un fichier CSV avec vos questions de test. **Modifiez les exemples ci-dessous** avec vos vraies questions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "create-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fichier de r√©f√©rence cr√©√©: ./data/reference/reference_qa.csv\n",
      "üìä 3 questions pr√©par√©es\n",
      "\n",
      "üìã Aper√ßu de vos questions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q001</td>\n",
       "      <td>Quelle est la date limite du premier livrable ...</td>\n",
       "      <td>Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q002</td>\n",
       "      <td>Quel est le budget allou√© au projet DataScienc...</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q003</td>\n",
       "      <td>Qui sont les participants √† la r√©union de plan...</td>\n",
       "      <td>Planning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id                                           question    category\n",
       "0        Q001  Quelle est la date limite du premier livrable ...  Management\n",
       "1        Q002  Quel est le budget allou√© au projet DataScienc...     Finance\n",
       "2        Q003  Qui sont les participants √† la r√©union de plan...    Planning"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Conseil: Ajoutez plus de questions repr√©sentatives de votre usage SmartRAG\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üìù PERSONNALISEZ ces questions avec vos vrais cas d'usage SmartRAG\n",
    "reference_questions = {\n",
    "    'question_id': ['Q001', 'Q002', 'Q003'],\n",
    "    \n",
    "    'question': [\n",
    "        \"Quelle est la date limite du premier livrable de la personne que Marc Dubois manage et qui est experte en Python ?\",\n",
    "        \"Quel est le budget allou√© au projet DataScience pour 2025 ?\",\n",
    "        \"Qui sont les participants √† la r√©union de planification du 15 janvier ?\"\n",
    "    ],\n",
    "    \n",
    "    'reference_answer': [\n",
    "        \"Sophie Martin, qui est manag√©e par Marc Dubois et experte en Python, doit livrer un premier PoC (Proof of Concept) pour le 1er mars 2025.\",\n",
    "        \"Le budget allou√© au projet DataScience pour 2025 est de 250 000 euros, r√©parti entre infrastructure (100k‚Ç¨) et d√©veloppement (150k‚Ç¨).\",\n",
    "        \"Les participants √† la r√©union de planification du 15 janvier sont : Marc Dubois (chef de projet), Sophie Martin (d√©veloppeur senior), Alice Dupont (data scientist), et Paul Moreau (product owner).\"\n",
    "    ],\n",
    "    \n",
    "    'expected_contexts': [\n",
    "        \"R√©pertoire √©quipe|||Compte-rendu de r√©union|||Note de cadrage projet\",\n",
    "        \"Document budg√©taire|||Plan financier 2025|||Validation direction\",\n",
    "        \"Calendrier r√©unions|||Liste participants|||Planning √©quipe\"\n",
    "    ],\n",
    "    \n",
    "    'category': ['Management', 'Finance', 'Planning'],\n",
    "    'difficulty': ['Moyen', 'Facile', 'Facile']\n",
    "}\n",
    "\n",
    "# Cr√©ation du DataFrame\n",
    "df_reference = pd.DataFrame(reference_questions)\n",
    "\n",
    "# Sauvegarde dans le bon r√©pertoire\n",
    "os.makedirs('./data/reference', exist_ok=True)\n",
    "reference_path = './data/reference/reference_qa.csv'\n",
    "df_reference.to_csv(reference_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ Fichier de r√©f√©rence cr√©√©: {reference_path}\")\n",
    "print(f\"üìä {len(df_reference)} questions pr√©par√©es\")\n",
    "\n",
    "# Aper√ßu\n",
    "print(\"\\nüìã Aper√ßu de vos questions:\")\n",
    "display(df_reference[['question_id', 'question', 'category']])\n",
    "\n",
    "print(\"\\nüí° Conseil: Ajoutez plus de questions repr√©sentatives de votre usage SmartRAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-langfuse",
   "metadata": {},
   "source": [
    "## üîó √âtape 3 : Connexion √† Langfuse et r√©cup√©ration des traces\n",
    "\n",
    "### 3.1 Test de connexion Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "test-langfuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connexion √† Langfuse...\n",
      "‚úÖ Connexion Langfuse r√©ussie!\n",
      "üåê URL: https://cloud.langfuse.com\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "print(\"üîó Connexion √† Langfuse...\")\n",
    "\n",
    "try:\n",
    "    # Connexion\n",
    "    langfuse_client = Langfuse(\n",
    "        public_key=LANGFUSE_PUBLIC_KEY,\n",
    "        secret_key=LANGFUSE_SECRET_KEY, \n",
    "        host=LANGFUSE_BASE_URL\n",
    "    )\n",
    "    \n",
    "    # Test simple\n",
    "    traces = langfuse_client.api.trace.list(limit=1)\n",
    "    \n",
    "    print(\"‚úÖ Connexion Langfuse r√©ussie!\")\n",
    "    print(f\"üåê URL: {LANGFUSE_BASE_URL}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion Langfuse: {e}\")\n",
    "    print(\"\\nüîß V√©rifications:\")\n",
    "    print(\"   1. Vos cl√©s LANGFUSE_PUBLIC_KEY et LANGFUSE_SECRET_KEY sont correctes\")\n",
    "    print(\"   2. L'URL Langfuse est accessible\")\n",
    "    print(\"   3. SmartRAG envoie bien des traces √† Langfuse\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-2",
   "metadata": {},
   "source": [
    "### 3.2 R√©cup√©ration des traces SmartRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fetch-traces",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Recherche des traces SmartRAG:\n",
      "   P√©riode: du 2025-08-25 12:36 au 2025-09-01 12:36\n",
      "   Projet: Tous les projets\n",
      "\n",
      "üîç R√©cup√©ration en cours...\n",
      "   Page 1: 12 traces SmartRAG trouv√©es\n",
      "\n",
      "üìä R√©sultat: 12 traces SmartRAG r√©cup√©r√©es\n",
      "‚úÖ Pr√™t pour l'√©tape suivante!\n"
     ]
    }
   ],
   "source": [
    "# D√©finition de la p√©riode\n",
    "end_date = datetime.now() \n",
    "start_date = end_date - timedelta(days=EVALUATION_TIMERANGE)\n",
    "\n",
    "print(f\"üìÖ Recherche des traces SmartRAG:\")\n",
    "print(f\"   P√©riode: du {start_date.strftime('%Y-%m-%d %H:%M')} au {end_date.strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"   Projet: {SMARTRAG_PROJECT_NAME or 'Tous les projets'}\")\n",
    "\n",
    "# R√©cup√©ration des traces\n",
    "all_traces = []\n",
    "page = 1\n",
    "total_found = 0\n",
    "\n",
    "print(\"\\nüîç R√©cup√©ration en cours...\")\n",
    "\n",
    "while page <= 10:  # Limite pour √©viter les boucles infinies\n",
    "    try:\n",
    "        traces_response = langfuse_client.api.trace.list(\n",
    "            page=page,\n",
    "            limit=50,\n",
    "            from_timestamp=start_date,\n",
    "            to_timestamp=end_date\n",
    "        )\n",
    "        \n",
    "        if not traces_response.data:\n",
    "            break\n",
    "            \n",
    "        page_traces = []\n",
    "        for trace in traces_response.data:\n",
    "            # Filtrage par projet SmartRAG si sp√©cifi√©\n",
    "            if SMARTRAG_PROJECT_NAME:\n",
    "                if not (trace.name and SMARTRAG_PROJECT_NAME.lower() in trace.name.lower()):\n",
    "                    continue\n",
    "            \n",
    "            page_traces.append(trace)\n",
    "        \n",
    "        all_traces.extend(page_traces)\n",
    "        total_found += len(page_traces)\n",
    "        \n",
    "        print(f\"   Page {page}: {len(page_traces)} traces SmartRAG trouv√©es\")\n",
    "        \n",
    "        page += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Erreur page {page}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüìä R√©sultat: {total_found} traces SmartRAG r√©cup√©r√©es\")\n",
    "\n",
    "if total_found == 0:\n",
    "    print(\"\\n‚ùå Aucune trace trouv√©e!\")\n",
    "    print(\"\\nüîß V√©rifications √† faire:\")\n",
    "    print(\"   1. SmartRAG envoie-t-il des traces √† Langfuse ?\")\n",
    "    print(\"   2. La p√©riode de recherche est-elle correcte ?\")\n",
    "    print(\"   3. Le nom du projet SmartRAG correspond-il ?\")\n",
    "    print(\"\\nüí° Conseil: Testez d'abord avec SMARTRAG_PROJECT_NAME = '' pour voir toutes les traces\")\n",
    "else:\n",
    "    print(f\"‚úÖ Pr√™t pour l'√©tape suivante!\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "bk0xf1vtwrb",
   "source": "# üî¨ CELLULE DE DIAGNOSTIC - Analyse rapide d'une trace exemple\n# Ex√©cutez cette cellule pour comprendre le format de vos traces SmartRAG\n\nif len(all_traces) > 0:\n    print(\"üîç === ANALYSE RAPIDE D'UNE TRACE EXEMPLE ===\\\\n\")\n    \n    sample_trace = all_traces[0]  # Prendre la premi√®re trace\n    \n    print(f\"üìã Trace ID: {getattr(sample_trace, 'id', 'N/A')}\")\n    print(f\"üìÖ Timestamp: {getattr(sample_trace, 'timestamp', 'N/A')}\")\n    print(f\"üè∑Ô∏è  Name: {getattr(sample_trace, 'name', 'N/A')}\")\n    print(f\"üë§ User ID: {getattr(sample_trace, 'user_id', 'N/A')}\")\n    \n    # Analyse d√©taill√©e de l'input\n    print(f\\\"\\\\nüì• INPUT:\\\")\n    input_data = getattr(sample_trace, 'input', None)\n    print(f\\\"   Type: {type(input_data)}\\\")\n    if input_data is not None:\n        if isinstance(input_data, dict):\n            print(f\\\"   Keys: {list(input_data.keys())}\\\")\n            for key, value in input_data.items():\n                print(f\\\"   {key}: {str(value)[:150]}...\\\")\n        else:\n            print(f\\\"   Contenu: {str(input_data)[:300]}...\\\")\n    else:\n        print(\\\"   ‚ùå Aucun input\\\")\n    \n    # Analyse d√©taill√©e de l'output  \n    print(f\\\"\\\\nüì§ OUTPUT:\\\")\n    output_data = getattr(sample_trace, 'output', None)\n    print(f\\\"   Type: {type(output_data)}\\\")\n    if output_data is not None:\n        if isinstance(output_data, dict):\n            print(f\\\"   Keys: {list(output_data.keys())}\\\")\n            for key, value in output_data.items():\n                print(f\\\"   {key}: {str(value)[:150]}...\\\")\n        else:\n            print(f\\\"   Contenu: {str(output_data)[:300]}...\\\")\n    else:\n        print(\\\"   ‚ùå Aucun output\\\")\n    \n    # Analyse des observations\n    observations = getattr(sample_trace, 'observations', [])\n    print(f\\\"\\\\nüìä OBSERVATIONS ({len(observations)}):\\\")\n    if observations:\n        for i, obs in enumerate(observations[:3]):  # Limiter √† 3 pour l'affichage\n            obs_name = getattr(obs, 'name', f'obs_{i}')\n            obs_type = getattr(obs, 'type', 'unknown')\n            print(f\\\"   [{i+1}] {obs_name} (type: {obs_type})\\\")\n            \n            obs_input = getattr(obs, 'input', None)\n            obs_output = getattr(obs, 'output', None)\n            \n            if obs_input:\n                print(f\\\"       Input: {str(obs_input)[:100]}...\\\")\n            if obs_output:\n                print(f\\\"       Output: {str(obs_output)[:100]}...\\\")\n    else:\n        print(\\\"   ‚ùå Aucune observation\\\")\n    \n    # Analyse des m√©tadonn√©es\n    metadata = getattr(sample_trace, 'metadata', {})\n    print(f\\\"\\\\nüìã METADATA:\\\")\n    if metadata and isinstance(metadata, dict):\n        print(f\\\"   Keys: {list(metadata.keys())}\\\")\n        for key, value in list(metadata.items())[:5]:  # Limiter √† 5\n            print(f\\\"   {key}: {str(value)[:100]}...\\\")\n    else:\n        print(\\\"   ‚ùå Aucune m√©tadonn√©e\\\")\n    \n    print(\\\"\\\\n\\\" + \\\"=\\\"*50)\n    print(\\\"üí° CONSEILS SELON VOTRE FORMAT:\\\")\n    print(\\\"1. Si la question est dans input['message'] ‚Üí OK\\\")  \n    print(\\\"2. Si la r√©ponse est dans output['answer'] ‚Üí OK\\\")\n    print(\\\"3. Si les donn√©es sont dans les observations ‚Üí Identifier le nom\\\")\n    print(\\\"4. Si format diff√©rent ‚Üí Ajuster la fonction d'extraction\\\")\n    \nelse:\n    print(\\\"‚ùå Aucune trace √† analyser\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "step4-processing",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è √âtape 4 : Traitement et correspondance des traces\n",
    "\n",
    "### 4.1 Extraction des composants RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-components",
   "metadata": {},
   "outputs": [],
   "source": "from difflib import SequenceMatcher\n\ndef extract_smartrag_components(trace):\n    \"\"\"Extrait question, r√©ponse et contextes d'une trace SmartRAG avec diagnostic avanc√©\"\"\"\n    \n    # üîç Diagnostic complet de la trace\n    trace_info = {\n        'trace_id': getattr(trace, 'id', 'unknown'),\n        'timestamp': getattr(trace, 'timestamp', None),\n        'name': getattr(trace, 'name', ''),\n        'user_id': getattr(trace, 'user_id', None),\n        'session_id': getattr(trace, 'session_id', None)\n    }\n    \n    # === EXTRACTION DE LA QUESTION ===\n    question = \"\"\n    question_source = \"non trouv√©e\"\n    \n    # Strat√©gie 1: Input direct\n    if hasattr(trace, 'input') and trace.input:\n        if isinstance(trace.input, str) and len(trace.input.strip()) > 0:\n            question = trace.input.strip()\n            question_source = \"input (string)\"\n        elif isinstance(trace.input, dict):\n            # Recherche dans les cl√©s courantes\n            for key in ['message', 'question', 'query', 'prompt', 'text', 'content']:\n                if key in trace.input and trace.input[key]:\n                    question = str(trace.input[key]).strip()\n                    question_source = f\"input['{key}']\"\n                    break\n    \n    # Strat√©gie 2: M√©tadonn√©es\n    if not question and hasattr(trace, 'metadata') and trace.metadata:\n        for key in ['question', 'query', 'user_input', 'message']:\n            if key in trace.metadata and trace.metadata[key]:\n                question = str(trace.metadata[key]).strip()\n                question_source = f\"metadata['{key}']\"\n                break\n    \n    # Strat√©gie 3: Premi√®re observation avec input\n    if not question and hasattr(trace, 'observations'):\n        for obs in trace.observations[:3]:  # Limiter aux 3 premi√®res\n            if hasattr(obs, 'input') and obs.input:\n                if isinstance(obs.input, str) and len(obs.input.strip()) > 10:\n                    question = obs.input.strip()\n                    question_source = f\"observation['{getattr(obs, 'name', 'unknown')}'].input\"\n                    break\n                elif isinstance(obs.input, dict):\n                    for key in ['message', 'question', 'query']:\n                        if key in obs.input and obs.input[key]:\n                            question = str(obs.input[key]).strip()\n                            question_source = f\"observation['{getattr(obs, 'name', 'unknown')}'].input['{key}']\"\n                            break\n                    if question:\n                        break\n    \n    # === EXTRACTION DE LA R√âPONSE ===\n    answer = \"\"\n    answer_source = \"non trouv√©e\"\n    \n    # Strat√©gie 1: Output direct\n    if hasattr(trace, 'output') and trace.output:\n        if isinstance(trace.output, str) and len(trace.output.strip()) > 0:\n            answer = trace.output.strip()\n            answer_source = \"output (string)\"\n        elif isinstance(trace.output, dict):\n            # Recherche dans les cl√©s courantes\n            for key in ['answer', 'response', 'result', 'content', 'text', 'message']:\n                if key in trace.output and trace.output[key]:\n                    answer = str(trace.output[key]).strip()\n                    answer_source = f\"output['{key}']\"\n                    break\n    \n    # Strat√©gie 2: Derni√®re observation avec output\n    if not answer and hasattr(trace, 'observations'):\n        for obs in reversed(trace.observations[-3:]):  # 3 derni√®res en ordre inverse\n            if hasattr(obs, 'output') and obs.output:\n                if isinstance(obs.output, str) and len(obs.output.strip()) > 10:\n                    answer = obs.output.strip()\n                    answer_source = f\"observation['{getattr(obs, 'name', 'unknown')}'].output\"\n                    break\n                elif isinstance(obs.output, dict):\n                    for key in ['answer', 'response', 'result', 'content', 'text']:\n                        if key in obs.output and obs.output[key]:\n                            answer = str(obs.output[key]).strip()\n                            answer_source = f\"observation['{getattr(obs, 'name', 'unknown')}'].output['{key}']\"\n                            break\n                    if answer:\n                        break\n    \n    # Strat√©gie 3: M√©tadonn√©es pour la r√©ponse\n    if not answer and hasattr(trace, 'metadata') and trace.metadata:\n        for key in ['answer', 'response', 'final_answer', 'result']:\n            if key in trace.metadata and trace.metadata[key]:\n                answer = str(trace.metadata[key]).strip()\n                answer_source = f\"metadata['{key}']\"\n                break\n    \n    # === EXTRACTION DES CONTEXTES ===\n    contexts = []\n    context_source = \"non trouv√©s\"\n    \n    # Recherche de contextes dans les observations\n    if hasattr(trace, 'observations'):\n        for obs in trace.observations:\n            obs_name = getattr(obs, 'name', '').lower()\n            \n            # Observations li√©es au retrieval\n            if any(keyword in obs_name for keyword in ['retrieve', 'search', 'context', 'document']):\n                if hasattr(obs, 'output') and obs.output:\n                    if isinstance(obs.output, list):\n                        contexts.extend([str(item) for item in obs.output])\n                        context_source = f\"observation['{obs_name}'].output\"\n                    elif isinstance(obs.output, dict) and 'documents' in obs.output:\n                        docs = obs.output['documents']\n                        if isinstance(docs, list):\n                            contexts.extend([str(doc) for doc in docs])\n                            context_source = f\"observation['{obs_name}'].output['documents']\"\n    \n    # === ASSEMBLAGE DU R√âSULTAT ===\n    result = {\n        'trace_id': trace_info['trace_id'],\n        'timestamp': trace_info['timestamp'],\n        'question': question,\n        'answer': answer,\n        'contexts': contexts,\n        'name': trace_info['name'],\n        'user_id': trace_info['user_id'],\n        'session_id': trace_info['session_id'],\n        \n        # üîç Informations de diagnostic\n        'diagnostic': {\n            'question_source': question_source,\n            'answer_source': answer_source,\n            'context_source': context_source,\n            'question_length': len(question),\n            'answer_length': len(answer),\n            'context_count': len(contexts),\n            'has_observations': hasattr(trace, 'observations') and len(getattr(trace, 'observations', [])) > 0,\n            'observation_count': len(getattr(trace, 'observations', [])),\n            'input_type': type(getattr(trace, 'input', None)).__name__,\n            'output_type': type(getattr(trace, 'output', None)).__name__\n        }\n    }\n    \n    return result\n\n# Traitement de toutes les traces avec diagnostic d√©taill√©\nprint(\"‚öôÔ∏è Traitement avanc√© des traces SmartRAG...\")\nprint(\"üîç Diagnostic des formats de traces activ√©\\n\")\n\nprocessed_traces = []\ndiagnostic_summary = {\n    'total_traces': len(all_traces),\n    'question_sources': {},\n    'answer_sources': {},\n    'context_sources': {},\n    'errors': []\n}\n\nfor i, trace in enumerate(all_traces):\n    try:\n        components = extract_smartrag_components(trace)\n        \n        # Collecte des statistiques de diagnostic\n        diag = components['diagnostic']\n        \n        # Comptage des sources\n        q_src = diag['question_source']\n        a_src = diag['answer_source']\n        c_src = diag['context_source']\n        \n        diagnostic_summary['question_sources'][q_src] = diagnostic_summary['question_sources'].get(q_src, 0) + 1\n        diagnostic_summary['answer_sources'][a_src] = diagnostic_summary['answer_sources'].get(a_src, 0) + 1\n        diagnostic_summary['context_sources'][c_src] = diagnostic_summary['context_sources'].get(c_src, 0) + 1\n        \n        # Traces valides (avec question ET r√©ponse)\n        if components['question'] and components['answer']:\n            processed_traces.append(components)\n            \n            # Affichage des premi√®res traces pour debug\n            if len(processed_traces) <= 3:\n                print(f\"‚úÖ Trace {i+1}: {components['trace_id'][:8]}...\")\n                print(f\"   Question: {diag['question_source']} ({diag['question_length']} chars)\")\n                print(f\"   R√©ponse: {diag['answer_source']} ({diag['answer_length']} chars)\")\n                print(f\"   Contextes: {diag['context_source']} ({diag['context_count']} items)\")\n                print()\n        \n    except Exception as e:\n        error_msg = f\"Trace {i+1}: {str(e)}\"\n        diagnostic_summary['errors'].append(error_msg)\n        if len(diagnostic_summary['errors']) <= 3:\n            print(f\"‚ö†Ô∏è  {error_msg}\")\n\nprint(f\"üìä === R√âSUM√â DU TRAITEMENT ===\")\nprint(f\"Total des traces analys√©es: {diagnostic_summary['total_traces']}\")\nprint(f\"‚úÖ Traces valides: {len(processed_traces)}\")\nprint(f\"‚ö†Ô∏è  Erreurs: {len(diagnostic_summary['errors'])}\")\n\nprint(f\"\\nüîç === DIAGNOSTIC DES FORMATS ===\")\nprint(\"Sources des QUESTIONS:\")\nfor source, count in sorted(diagnostic_summary['question_sources'].items(), key=lambda x: x[1], reverse=True):\n    print(f\"   {source}: {count} traces\")\n\nprint(\"\\nSources des R√âPONSES:\")\nfor source, count in sorted(diagnostic_summary['answer_sources'].items(), key=lambda x: x[1], reverse=True):\n    print(f\"   {source}: {count} traces\")\n\nprint(\"\\nSources des CONTEXTES:\")\nfor source, count in sorted(diagnostic_summary['context_sources'].items(), key=lambda x: x[1], reverse=True):\n    print(f\"   {source}: {count} traces\")\n\n# Aper√ßu des traces trait√©es\nif len(processed_traces) > 0:\n    df_traces = pd.DataFrame([{\n        'trace_id': t['trace_id'][:8] + '...',\n        'question': t['question'][:80] + '...' if len(t['question']) > 80 else t['question'],\n        'answer_length': len(t['answer']),\n        'question_src': t['diagnostic']['question_source'],\n        'answer_src': t['diagnostic']['answer_source']\n    } for t in processed_traces])\n    \n    print(\"\\nüìã Aper√ßu des traces valides:\")\n    display(df_traces.head())\n    \n    print(f\"\\nüí° Conseil: {len(processed_traces)} traces SmartRAG pr√™tes pour l'√©valuation Ragas!\")\n    \nelse:\n    print(\"\\n‚ùå Aucune trace SmartRAG valide trouv√©e!\")\n    print(\"\\nLes traces doivent contenir √† la fois une question et une r√©ponse.\")\n    \n    print(\"\\nüîß SOLUTIONS selon le diagnostic:\")\n    \n    # Suggestions bas√©es sur les sources trouv√©es\n    most_common_q = max(diagnostic_summary['question_sources'].items(), key=lambda x: x[1], default=('non trouv√©e', 0))\n    most_common_a = max(diagnostic_summary['answer_sources'].items(), key=lambda x: x[1], default=('non trouv√©e', 0))\n    \n    if most_common_q[0] != 'non trouv√©e' and most_common_a[0] == 'non trouv√©e':\n        print(f\"   ‚Ä¢ Questions trouv√©es via {most_common_q[0]} mais pas de r√©ponses\")\n        print(f\"   ‚Ä¢ V√©rifiez que SmartRAG enregistre bien les r√©ponses g√©n√©r√©es\")\n        print(f\"   ‚Ä¢ La r√©ponse pourrait √™tre dans une autre structure de la trace\")\n    \n    elif most_common_q[0] == 'non trouv√©e' and most_common_a[0] != 'non trouv√©e':\n        print(f\"   ‚Ä¢ R√©ponses trouv√©es via {most_common_a[0]} mais pas de questions\")\n        print(f\"   ‚Ä¢ V√©rifiez que SmartRAG enregistre bien les questions utilisateur\")\n        \n    elif most_common_q[0] == 'non trouv√©e' and most_common_a[0] == 'non trouv√©e':\n        print(f\"   ‚Ä¢ Ni questions ni r√©ponses d√©tect√©es dans le format attendu\")\n        print(f\"   ‚Ä¢ Le format de trace SmartRAG pourrait √™tre diff√©rent du standard\")\n        print(f\"   ‚Ä¢ Examinez manuellement une trace avec la cellule de diagnostic\")\n        \n    print(\"\\nüìß Si le probl√®me persiste, partagez le diagnostic avec l'√©quipe technique SmartRAG\")"
  },
  {
   "cell_type": "markdown",
   "id": "step4-2",
   "metadata": {},
   "source": [
    "### 4.2 Correspondance avec vos questions de r√©f√©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "match-questions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"Calcule la similarit√© entre deux questions\"\"\"\n",
    "    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "# Correspondance des questions\n",
    "print(\"üîó Correspondance des questions de r√©f√©rence avec les traces SmartRAG...\")\n",
    "\n",
    "matched_pairs = []\n",
    "similarity_threshold = 0.6  # Seuil de similarit√© (ajustable)\n",
    "\n",
    "for _, ref_row in df_reference.iterrows():\n",
    "    ref_question = ref_row['question']\n",
    "    best_match = None\n",
    "    best_similarity = 0\n",
    "    \n",
    "    # Recherche de la meilleure correspondance\n",
    "    for trace in processed_traces:\n",
    "        similarity = calculate_similarity(ref_question, trace['question'])\n",
    "        \n",
    "        if similarity > best_similarity and similarity >= similarity_threshold:\n",
    "            best_similarity = similarity\n",
    "            best_match = trace\n",
    "    \n",
    "    if best_match:\n",
    "        match_data = {\n",
    "            'question_id': ref_row['question_id'],\n",
    "            'reference_question': ref_row['question'],\n",
    "            'reference_answer': ref_row['reference_answer'],\n",
    "            'category': ref_row['category'],\n",
    "            'difficulty': ref_row['difficulty'],\n",
    "            \n",
    "            'trace_id': best_match['trace_id'],\n",
    "            'actual_question': best_match['question'],\n",
    "            'actual_answer': best_match['answer'],\n",
    "            'retrieved_contexts': best_match['contexts'],\n",
    "            'question_similarity': best_similarity,\n",
    "            \n",
    "            'timestamp': best_match['timestamp'],\n",
    "            'session_id': best_match.get('session_id'),\n",
    "            'user_id': best_match.get('user_id')\n",
    "        }\n",
    "        matched_pairs.append(match_data)\n",
    "        \n",
    "        print(f\"   ‚úÖ {ref_row['question_id']}: Similarit√© {best_similarity:.2%}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {ref_row['question_id']}: Aucune correspondance (seuil: {similarity_threshold:.0%})\")\n",
    "\n",
    "df_matched = pd.DataFrame(matched_pairs)\n",
    "\n",
    "print(f\"\\nüìä R√©sultat des correspondances:\")\n",
    "print(f\"   üéØ {len(df_matched)}/{len(df_reference)} questions associ√©es\")\n",
    "\n",
    "if len(df_matched) > 0:\n",
    "    avg_similarity = df_matched['question_similarity'].mean()\n",
    "    print(f\"   üìà Similarit√© moyenne: {avg_similarity:.1%}\")\n",
    "    \n",
    "    print(\"\\nüìã Questions trouv√©es:\")\n",
    "    display(df_matched[['question_id', 'question_similarity', 'category']])\n",
    "    \n",
    "    if avg_similarity < 0.8:\n",
    "        print(f\"\\nüí° Conseil: Similarit√© faible ({avg_similarity:.1%}). V√©rifiez que vos questions de r√©f√©rence correspondent bien aux questions pos√©es √† SmartRAG.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Aucune correspondance trouv√©e!\")\n",
    "    print(\"\\nüîß Solutions possibles:\")\n",
    "    print(f\"   1. R√©duire le seuil: similarity_threshold = 0.4 (actuellement {similarity_threshold})\")\n",
    "    print(\"   2. Ajuster vos questions de r√©f√©rence pour qu'elles correspondent mieux\")\n",
    "    print(\"   3. V√©rifier que SmartRAG re√ßoit bien ces questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-ragas",
   "metadata": {},
   "source": [
    "## üéØ √âtape 5 : √âvaluation Ragas\n",
    "\n",
    "### 5.1 Configuration du mod√®le d'√©valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-ragas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des mod√®les pour Ragas selon le provider choisi\n",
    "print(f\"ü§ñ Configuration du mod√®le d'√©valuation: {RAGAS_LLM_PROVIDER} - {RAGAS_MODEL_NAME}\")\n",
    "\n",
    "if RAGAS_LLM_PROVIDER == \"openai\":\n",
    "    import openai\n",
    "    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "    \n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    \n",
    "    # Configuration Ragas avec GPT-4.1-mini\n",
    "    ragas_llm = ChatOpenAI(\n",
    "        model=RAGAS_MODEL_NAME,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    ragas_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    print(f\"‚úÖ OpenAI configur√©: {RAGAS_MODEL_NAME}\")\n",
    "    \n",
    "elif RAGAS_LLM_PROVIDER == \"gemini\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "    \n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "    \n",
    "    ragas_llm = ChatGoogleGenerativeAI(\n",
    "        model=RAGAS_MODEL_NAME,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    ragas_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    \n",
    "    print(f\"‚úÖ Gemini configur√©: {RAGAS_MODEL_NAME}\")\n",
    "    \n",
    "elif RAGAS_LLM_PROVIDER == \"claude\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    from langchain_openai import OpenAIEmbeddings  # Claude utilise OpenAI pour les embeddings\n",
    "    \n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY  # N√©cessaire pour les embeddings\n",
    "    \n",
    "    ragas_llm = ChatAnthropic(\n",
    "        model=RAGAS_MODEL_NAME,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    ragas_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    print(f\"‚úÖ Claude configur√©: {RAGAS_MODEL_NAME}\")\n",
    "\n",
    "# Test rapide du mod√®le\n",
    "try:\n",
    "    test_response = ragas_llm.invoke(\"R√©pondez simplement: OK\")\n",
    "    print(f\"üîó Test de connexion: {test_response.content[:20]}...\")\n",
    "    print(\"‚úÖ Mod√®le d'√©valuation pr√™t!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion au mod√®le: {e}\")\n",
    "    print(\"\\nüîß V√©rifications:\")\n",
    "    print(f\"   1. Votre cl√© API {RAGAS_LLM_PROVIDER.upper()} est correcte\")\n",
    "    print(f\"   2. Le mod√®le {RAGAS_MODEL_NAME} est accessible\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-2",
   "metadata": {},
   "source": [
    "### 5.2 Lancement de l'√©valuation Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "\n",
    "if len(df_matched) == 0:\n",
    "    print(\"‚ùå Impossible d'√©valuer: aucune question correspondante trouv√©e.\")\n",
    "    print(\"Revenez √† l'√©tape pr√©c√©dente pour ajuster les correspondances.\")\n",
    "else:\n",
    "    print(f\"üéØ √âvaluation Ragas de {len(df_matched)} questions avec {RAGAS_MODEL_NAME}...\")\n",
    "    \n",
    "    # Pr√©paration des donn√©es pour Ragas\n",
    "    eval_questions = df_matched['reference_question'].tolist()\n",
    "    eval_answers = df_matched['actual_answer'].tolist()\n",
    "    eval_contexts = [[\"Contexte SmartRAG\"] for _ in range(len(df_matched))]  # Placeholder\n",
    "    eval_ground_truths = df_matched['reference_answer'].tolist()\n",
    "    \n",
    "    # Cr√©ation du dataset Ragas\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"question\": eval_questions,\n",
    "        \"answer\": eval_answers, \n",
    "        \"contexts\": eval_contexts,\n",
    "        \"ground_truth\": eval_ground_truths\n",
    "    })\n",
    "    \n",
    "    # M√©triques √† √©valuer\n",
    "    metrics = [\n",
    "        faithfulness,      # Fid√©lit√© au contexte\n",
    "        answer_relevancy,  # Pertinence de la r√©ponse \n",
    "        context_precision, # Pr√©cision du contexte\n",
    "        context_recall     # Rappel du contexte\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # üöÄ √âVALUATION RAGAS\n",
    "        print(\"‚è≥ √âvaluation en cours... (peut prendre 1-3 minutes)\")\n",
    "        \n",
    "        results = evaluate(\n",
    "            dataset=eval_dataset,\n",
    "            metrics=metrics,\n",
    "            llm=ragas_llm,\n",
    "            embeddings=ragas_embeddings\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüéâ √âvaluation termin√©e!\")\n",
    "        \n",
    "        # Affichage des r√©sultats\n",
    "        print(\"\\nüìä === R√âSULTATS RAGAS ===\")\n",
    "        print(f\"Mod√®le utilis√©: {RAGAS_LLM_PROVIDER} ({RAGAS_MODEL_NAME})\")\n",
    "        print()\n",
    "        \n",
    "        for metric_name, score in results.items():\n",
    "            if isinstance(score, (list, pd.Series)):\n",
    "                avg_score = sum(score) / len(score) if score else 0\n",
    "                print(f\"üìà {metric_name:18}: {avg_score:.4f} (moyenne)\")\n",
    "            else:\n",
    "                print(f\"üìà {metric_name:18}: {score:.4f}\")\n",
    "        \n",
    "        # Fusion avec les donn√©es originales\n",
    "        df_results = df_matched.copy()\n",
    "        \n",
    "        for metric_name, scores in results.items():\n",
    "            if isinstance(scores, (list, pd.Series)):\n",
    "                df_results[metric_name] = scores\n",
    "            else:\n",
    "                df_results[metric_name] = [scores] * len(df_results)\n",
    "        \n",
    "        print(f\"\\n‚úÖ √âvaluation de {len(df_results)} questions termin√©e!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur pendant l'√©valuation: {e}\")\n",
    "        print(\"\\nüîß V√©rifications possibles:\")\n",
    "        print(\"   1. Connexion internet stable\")\n",
    "        print(\"   2. Cr√©dits API suffisants\")\n",
    "        print(\"   3. Mod√®le disponible et accessible\")\n",
    "        \n",
    "        # Cr√©er un DataFrame vide pour continuer\n",
    "        df_results = df_matched.copy()\n",
    "        for metric in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:\n",
    "            df_results[metric] = None\n",
    "        \n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-analysis",
   "metadata": {},
   "source": [
    "## üìä √âtape 6 : Analyse des r√©sultats\n",
    "\n",
    "### 6.1 Visualisation des scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "if 'df_results' in locals() and len(df_results) > 0:\n",
    "    \n",
    "    # M√©triques Ragas disponibles\n",
    "    ragas_metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "    available_metrics = [m for m in ragas_metrics if m in df_results.columns and df_results[m].notna().any()]\n",
    "    \n",
    "    if available_metrics:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'üìä R√©sultats √âvaluation SmartRAG - {RAGAS_MODEL_NAME}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Graphique 1: Scores moyens\n",
    "        ax1 = axes[0, 0]\n",
    "        means = [df_results[m].mean() for m in available_metrics]\n",
    "        colors = sns.color_palette(\"husl\", len(available_metrics))\n",
    "        bars = ax1.bar(available_metrics, means, color=colors)\n",
    "        ax1.set_title('üéØ Scores Moyens par M√©trique')\n",
    "        ax1.set_ylabel('Score (0-1)')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Ajout des valeurs sur les barres\n",
    "        for bar, mean in zip(bars, means):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                    f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Graphique 2: Distribution faithfulness\n",
    "        ax2 = axes[0, 1]\n",
    "        if 'faithfulness' in available_metrics:\n",
    "            faithfulness_scores = df_results['faithfulness'].dropna()\n",
    "            ax2.hist(faithfulness_scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            ax2.set_title('üìà Distribution Faithfulness')\n",
    "            ax2.set_xlabel('Score')\n",
    "            ax2.set_ylabel('Fr√©quence')\n",
    "            ax2.axvline(faithfulness_scores.mean(), color='red', linestyle='--', \n",
    "                       label=f'Moyenne: {faithfulness_scores.mean():.3f}')\n",
    "            ax2.legend()\n",
    "        \n",
    "        # Graphique 3: Scores par cat√©gorie\n",
    "        ax3 = axes[1, 0]\n",
    "        if 'category' in df_results.columns and len(df_results['category'].unique()) > 1:\n",
    "            category_scores = df_results.groupby('category')['faithfulness'].mean().sort_values(ascending=True)\n",
    "            category_scores.plot(kind='barh', ax=ax3, color='lightgreen')\n",
    "            ax3.set_title('üìÇ Performance par Cat√©gorie')\n",
    "            ax3.set_xlabel('Score Faithfulness Moyen')\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'Pas assez de cat√©gories\\npour l\\'analyse', \n",
    "                    ha='center', va='center', transform=ax3.transAxes, fontsize=12)\n",
    "            ax3.set_title('üìÇ Analyse par Cat√©gorie')\n",
    "        \n",
    "        # Graphique 4: Corr√©lation similarit√© vs performance\n",
    "        ax4 = axes[1, 1]\n",
    "        if 'question_similarity' in df_results.columns and 'answer_relevancy' in available_metrics:\n",
    "            x = df_results['question_similarity']\n",
    "            y = df_results['answer_relevancy']\n",
    "            ax4.scatter(x, y, alpha=0.7, s=60, color='purple')\n",
    "            ax4.set_xlabel('Similarit√© Question')\n",
    "            ax4.set_ylabel('Answer Relevancy')\n",
    "            ax4.set_title('üîó Similarit√© vs Performance')\n",
    "            \n",
    "            # Ligne de tendance\n",
    "            z = np.polyfit(x, y, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax4.plot(x, p(x), \"r--\", alpha=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistiques d√©taill√©es\n",
    "        print(\"\\nüìä === ANALYSE D√âTAILL√âE ===\")\n",
    "        \n",
    "        for metric in available_metrics:\n",
    "            values = df_results[metric].dropna()\n",
    "            if len(values) > 0:\n",
    "                print(f\"\\nüìà {metric.upper()}:\")\n",
    "                print(f\"   Moyenne: {values.mean():.4f}\")\n",
    "                print(f\"   M√©diane: {values.median():.4f}\")\n",
    "                print(f\"   Min-Max: {values.min():.4f} - {values.max():.4f}\")\n",
    "                \n",
    "                # Questions probl√©matiques\n",
    "                if values.min() < 0.5:\n",
    "                    worst_idx = values.idxmin()\n",
    "                    worst_question = df_results.loc[worst_idx, 'question_id']\n",
    "                    print(f\"   üî¥ Plus faible: {worst_question} ({values.min():.3f})\")\n",
    "                \n",
    "                # Meilleures questions\n",
    "                if values.max() > 0.7:\n",
    "                    best_idx = values.idxmax()\n",
    "                    best_question = df_results.loc[best_idx, 'question_id']\n",
    "                    print(f\"   üü¢ Meilleure: {best_question} ({values.max():.3f})\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Aucune m√©trique Ragas disponible pour la visualisation\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Aucun r√©sultat √† analyser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-2",
   "metadata": {},
   "source": [
    "### 6.2 Recommandations personnalis√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_results' in locals() and len(df_results) > 0 and available_metrics:\n",
    "    \n",
    "    print(\"üéØ === RECOMMANDATIONS POUR VOTRE SMARTRAG ===\")\n",
    "    print()\n",
    "    \n",
    "    # Analyse de la Faithfulness\n",
    "    if 'faithfulness' in available_metrics:\n",
    "        faith_mean = df_results['faithfulness'].mean()\n",
    "        print(f\"üîç FAITHFULNESS (Fid√©lit√©): {faith_mean:.3f}\")\n",
    "        \n",
    "        if faith_mean < 0.5:\n",
    "            print(\"   üî¥ CRITIQUE: Vos r√©ponses ne sont pas fid√®les aux contextes\")\n",
    "            print(\"   üí° Actions:\")\n",
    "            print(\"      - V√©rifiez la qualit√© des documents index√©s\")\n",
    "            print(\"      - Am√©liorez le chunking (taille, overlap)\")\n",
    "            print(\"      - Ajustez le prompt pour rester fid√®le aux sources\")\n",
    "        elif faith_mean < 0.7:\n",
    "            print(\"   üü° MOYEN: Am√©lioration possible de la fid√©lit√©\")\n",
    "            print(\"   üí° Actions:\")\n",
    "            print(\"      - Testez diff√©rents param√®tres de reranking\")\n",
    "            print(\"      - Optimisez le nombre de contextes r√©cup√©r√©s\")\n",
    "        else:\n",
    "            print(\"   üü¢ EXCELLENT: Vos r√©ponses sont fid√®les aux sources!\")\n",
    "    \n",
    "    # Analyse de Answer Relevancy\n",
    "    if 'answer_relevancy' in available_metrics:\n",
    "        rel_mean = df_results['answer_relevancy'].mean()\n",
    "        print(f\"\\nüí¨ ANSWER RELEVANCY (Pertinence): {rel_mean:.3f}\")\n",
    "        \n",
    "        if rel_mean < 0.6:\n",
    "            print(\"   üî¥ CRITIQUE: Vos r√©ponses ne r√©pondent pas bien aux questions\")\n",
    "            print(\"   üí° Actions:\")\n",
    "            print(\"      - Am√©liorez votre prompt de g√©n√©ration\")\n",
    "            print(\"      - Testez un mod√®le LLM plus performant\")\n",
    "            print(\"      - Ajoutez des exemples dans le prompt\")\n",
    "        elif rel_mean < 0.8:\n",
    "            print(\"   üü° CORRECT: Place √† l'am√©lioration de la pertinence\")\n",
    "            print(\"   üí° Actions:\")\n",
    "            print(\"      - Affinez les instructions du prompt\")\n",
    "            print(\"      - Testez diff√©rentes temp√©ratures\")\n",
    "        else:\n",
    "            print(\"   üü¢ EXCELLENT: Vos r√©ponses sont tr√®s pertinentes!\")\n",
    "    \n",
    "    # Analyse Context Precision\n",
    "    if 'context_precision' in available_metrics:\n",
    "        prec_mean = df_results['context_precision'].mean()\n",
    "        print(f\"\\nüéØ CONTEXT PRECISION (Pr√©cision): {prec_mean:.3f}\")\n",
    "        \n",
    "        if prec_mean < 0.5:\n",
    "            print(\"   üî¥ CRITIQUE: Trop de contextes non-pertinents r√©cup√©r√©s\")\n",
    "            print(\"   üí° Actions:\")\n",
    "            print(\"      - Am√©liorez votre strat√©gie de retrieval\")\n",
    "            print(\"      - Ajustez les seuils de similarit√©\")\n",
    "            print(\"      - Utilisez un meilleur mod√®le d'embedding\")\n",
    "        elif prec_mean < 0.7:\n",
    "            print(\"   üü° CORRECT: La pr√©cision du retrieval peut √™tre am√©lior√©e\")\n",
    "            print(\"   üí° Actions:\")\n",
    "            print(\"      - Testez diff√©rents mod√®les d'embedding\")\n",
    "            print(\"      - Optimisez le reranking\")\n",
    "        else:\n",
    "            print(\"   üü¢ EXCELLENT: Vos contextes sont tr√®s pr√©cis!\")\n",
    "    \n",
    "    # Analyse par difficult√©\n",
    "    if 'difficulty' in df_results.columns:\n",
    "        print(\"\\nüìä PERFORMANCE PAR DIFFICULT√â:\")\n",
    "        diff_analysis = df_results.groupby('difficulty').agg({\n",
    "            'faithfulness': 'mean',\n",
    "            'answer_relevancy': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        for diff, scores in diff_analysis.iterrows():\n",
    "            print(f\"   {diff}: Fid√©lit√© {scores['faithfulness']:.3f}, Pertinence {scores['answer_relevancy']:.3f}\")\n",
    "    \n",
    "    # Recommandations g√©n√©rales\n",
    "    print(\"\\nüöÄ PROCHAINES √âTAPES RECOMMAND√âES:\")\n",
    "    print(\"\\n1. üìä MONITORING CONTINU\")\n",
    "    print(\"   - Lancez cette √©valuation chaque semaine\")\n",
    "    print(\"   - Suivez l'√©volution des m√©triques\")\n",
    "    print(\"   - Alertez si les scores baissent\")\n",
    "    \n",
    "    print(\"\\n2. üîß OPTIMISATION TECHNIQUE\")\n",
    "    print(\"   - A/B testez diff√©rentes configurations SmartRAG\")\n",
    "    print(\"   - Comparez plusieurs mod√®les LLM\")\n",
    "    print(\"   - Testez diff√©rentes strat√©gies de chunking\")\n",
    "    \n",
    "    print(\"\\n3. üìà AM√âLIORATION CONTINUE\")\n",
    "    print(\"   - Ajoutez plus de questions de r√©f√©rence\")\n",
    "    print(\"   - Analysez les √©checs pour comprendre les lacunes\")\n",
    "    print(\"   - Collectez les retours utilisateurs\")\n",
    "    \n",
    "    print(\"\\nüí° CONSEIL: Exportez ces r√©sultats pour un suivi historique!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Impossible de g√©n√©rer des recommandations sans r√©sultats d'√©valuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-export",
   "metadata": {},
   "source": [
    "## üíæ √âtape 7 : Export et sauvegarde\n",
    "\n",
    "### 7.1 Sauvegarde des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_results' in locals() and len(df_results) > 0:\n",
    "    \n",
    "    # Cr√©ation du dossier de r√©sultats\n",
    "    os.makedirs('./data/results', exist_ok=True)\n",
    "    \n",
    "    # Nom de fichier avec timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_file = f'./data/results/smartrag_evaluation_{timestamp}.csv'\n",
    "    json_file = f'./data/results/smartrag_evaluation_{timestamp}_detailed.json'\n",
    "    \n",
    "    print(f\"üíæ Sauvegarde des r√©sultats...\")\n",
    "    \n",
    "    # Export CSV\n",
    "    export_columns = [\n",
    "        'question_id', 'category', 'difficulty', 'question_similarity',\n",
    "        'reference_question', 'actual_question', 'reference_answer', 'actual_answer'\n",
    "    ] + available_metrics + [\n",
    "        'trace_id', 'timestamp', 'session_id', 'user_id'\n",
    "    ]\n",
    "    \n",
    "    # Filtrage des colonnes existantes\n",
    "    final_columns = [col for col in export_columns if col in df_results.columns]\n",
    "    df_export = df_results[final_columns].copy()\n",
    "    \n",
    "    df_export.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Export JSON d√©taill√©\n",
    "    json_data = {\n",
    "        'evaluation_metadata': {\n",
    "            'evaluation_date': datetime.now().isoformat(),\n",
    "            'evaluation_period': {\n",
    "                'start': start_date.isoformat(),\n",
    "                'end': end_date.isoformat(),\n",
    "                'days': EVALUATION_TIMERANGE\n",
    "            },\n",
    "            'smartrag_project': SMARTRAG_PROJECT_NAME or 'All projects',\n",
    "            'langfuse_url': LANGFUSE_BASE_URL,\n",
    "            'evaluation_model': {\n",
    "                'provider': RAGAS_LLM_PROVIDER,\n",
    "                'model': RAGAS_MODEL_NAME\n",
    "            },\n",
    "            'total_reference_questions': len(df_reference),\n",
    "            'matched_questions': len(df_results),\n",
    "            'match_rate': len(df_results) / len(df_reference)\n",
    "        },\n",
    "        'summary_metrics': {},\n",
    "        'detailed_results': df_results.to_dict('records')\n",
    "    }\n",
    "    \n",
    "    # Calcul des m√©triques de r√©sum√©\n",
    "    for metric in available_metrics:\n",
    "        values = df_results[metric].dropna()\n",
    "        if len(values) > 0:\n",
    "            json_data['summary_metrics'][metric] = {\n",
    "                'mean': float(values.mean()),\n",
    "                'median': float(values.median()),\n",
    "                'std': float(values.std()) if len(values) > 1 else None,\n",
    "                'min': float(values.min()),\n",
    "                'max': float(values.max()),\n",
    "                'count': int(len(values))\n",
    "            }\n",
    "    \n",
    "    # Sauvegarde JSON\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    print(f\"\\n‚úÖ R√©sultats sauvegard√©s:\")\n",
    "    print(f\"   üìÑ CSV: {csv_file}\")\n",
    "    print(f\"   üìã JSON: {json_file}\")\n",
    "    print(f\"   üìä {len(df_export)} questions √©valu√©es\")\n",
    "    print(f\"   üìà {len(final_columns)} colonnes export√©es\")\n",
    "    \n",
    "    # Aper√ßu des donn√©es export√©es\n",
    "    print(\"\\nüìã Aper√ßu des r√©sultats export√©s:\")\n",
    "    display_columns = ['question_id', 'category'] + [m for m in available_metrics if m in df_export.columns][:3]\n",
    "    display(df_export[display_columns].head())\n",
    "    \n",
    "    # Statistiques finales\n",
    "    print(\"\\nüéØ === R√âSUM√â FINAL ===\")\n",
    "    print(f\"Provider d'√©valuation: {RAGAS_LLM_PROVIDER} - {RAGAS_MODEL_NAME}\")\n",
    "    print(f\"Questions √©valu√©es: {len(df_export)}/{len(df_reference)}\")\n",
    "    \n",
    "    if available_metrics:\n",
    "        print(\"\\nScores moyens finaux:\")\n",
    "        for metric in available_metrics:\n",
    "            score = df_results[metric].mean()\n",
    "            status = \"üü¢\" if score > 0.7 else \"üü°\" if score > 0.5 else \"üî¥\"\n",
    "            print(f\"   {status} {metric}: {score:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Aucune donn√©e √† exporter\")\n",
    "    print(\"\\nAssurez-vous d'avoir:\")\n",
    "    print(\"   1. Des traces SmartRAG dans Langfuse\")\n",
    "    print(\"   2. Des questions de r√©f√©rence correspondantes\")\n",
    "    print(\"   3. Une √©valuation Ragas r√©ussie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## üéâ F√©licitations ! √âvaluation termin√©e\n",
    "\n",
    "Vous avez maintenant une √©valuation compl√®te de votre syst√®me SmartRAG !\n",
    "\n",
    "### üìà Ce que vous avez accompli :\n",
    "1. ‚úÖ **Configur√©** l'√©valuation avec les mod√®les 2025\n",
    "2. ‚úÖ **R√©cup√©r√©** vos traces SmartRAG depuis Langfuse  \n",
    "3. ‚úÖ **√âvalu√©** la qualit√© avec Ragas\n",
    "4. ‚úÖ **Analys√©** les performances par m√©trique\n",
    "5. ‚úÖ **Export√©** les r√©sultats pour suivi\n",
    "\n",
    "### üöÄ Prochaines √©tapes recommand√©es :\n",
    "\n",
    "**üîÑ Automatisation**\n",
    "- Planifiez cette √©valuation chaque semaine\n",
    "- Cr√©ez des alertes si les scores baissent\n",
    "- Int√©grez dans votre CI/CD\n",
    "\n",
    "**üìä Suivi continu**\n",
    "- Comparez les r√©sultats dans le temps\n",
    "- Testez diff√©rentes configurations SmartRAG\n",
    "- A/B testez plusieurs mod√®les LLM\n",
    "\n",
    "**üéØ Am√©lioration**\n",
    "- Ajoutez plus de questions repr√©sentatives\n",
    "- Analysez les questions avec les plus mauvais scores\n",
    "- Optimisez votre pipeline RAG selon les r√©sultats\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Conseils pour aller plus loin :\n",
    "\n",
    "1. **Diversifiez vos tests** : Ajoutez des questions de diff√©rentes cat√©gories et difficult√©s\n",
    "2. **Surveillez les tendances** : Utilisez les exports JSON pour analyser l'√©volution\n",
    "3. **Comparez les mod√®les** : Testez GPT-4.1-mini vs Gemini 2.5 vs Claude 3.5\n",
    "4. **Optimisez selon les m√©triques** :\n",
    "   - **Faithfulness faible** ‚Üí Am√©liorer le retrieval\n",
    "   - **Answer relevancy faible** ‚Üí Optimiser le prompt\n",
    "   - **Context precision faible** ‚Üí Ajuster les seuils\n",
    "\n",
    "**üéØ Votre SmartRAG est maintenant sous surveillance qualit√© !**\n",
    "\n",
    "---\n",
    "\n",
    "*üíæ N'oubliez pas de sauvegarder ce notebook avec vos configurations pour reproduire l'√©valuation facilement.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}