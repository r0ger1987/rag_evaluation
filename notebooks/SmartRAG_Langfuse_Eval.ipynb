{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# Évaluation SmartRAG via traces Langfuse + Ragas\n",
    "Ce notebook récupère les traces SmartRAG stockées dans Langfuse, les compare avec un jeu de données de référence (CSV),\n",
    "et calcule les métriques Ragas pour évaluer différentes configurations SmartRAG.\n",
    "\n",
    "**Prérequis** : \n",
    "- SmartRAG configuré et intégré avec Langfuse\n",
    "- Traces SmartRAG disponibles dans Langfuse\n",
    "- Fichier CSV avec questions/réponses de référence\n",
    "- Python 3.10+, packages listés ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependencies",
   "metadata": {},
   "source": [
    "## 0) Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (dernières versions 2025)\n",
    "# %pip install --upgrade pip\n",
    "# %pip install langfuse>=3.0.0 ragas>=0.3.0 pandas matplotlib seaborn python-dotenv\n",
    "# %pip install openai>=1.0.0 google-generativeai>=0.8.0 anthropic>=0.64.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "## 1) Configuration\n",
    "Configurez vos variables d'environnement pour Langfuse et les chemins de fichiers.\n",
    "\n",
    "**Variables principales :**\n",
    "- `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, `LANGFUSE_BASE_URL` : Connexion Langfuse\n",
    "- `REFERENCE_CSV` : Chemin vers le fichier CSV avec questions/réponses de référence\n",
    "- `OUTPUT_CSV` : Fichier de sortie avec les résultats d'évaluation\n",
    "- `SMARTRAG_PROJECT_NAME` : Nom du projet SmartRAG dans Langfuse (optionnel)\n",
    "- `EVALUATION_TIMERANGE` : Période d'évaluation en jours (défaut: 7 derniers jours)\n",
    "- `RAGAS_LLM_PROVIDER` : Provider LLM pour Ragas (openai|gemini|claude|ollama)\n",
    "- `RAGAS_MODEL_NAME` : Modèle à utiliser (gpt-4.1-mini, gemini-2.5-flash, claude-3-5-haiku-20241022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config-vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration chargée:\n",
      "LANGFUSE_BASE_URL: https://cloud.langfuse.com\n",
      "REFERENCE_CSV: ./reference_qa_real.csv\n",
      "OUTPUT_CSV: ./smartrag_evaluation_results.csv\n",
      "SMARTRAG_PROJECT_NAME: Tous les projets\n",
      "EVALUATION_TIMERANGE: 7 jours\n",
      "RAGAS_LLM_PROVIDER: openai  # openai | gemini | claude | ollama\n",
      "RAGAS_MODEL_NAME: gpt-4.1-mini  # ou gpt-4.1-mini, gemini-2.5-flash, claude-3-5-haiku-20241022, etc.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration Langfuse ---\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-b65665b3-76ec-4a73-95ac-91254ae9af8a\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\", \"your-openai-api-key\")\n",
    "LANGFUSE_BASE_URL = os.getenv(\"LANGFUSE_BASE_URL\", \"https://cloud.langfuse.com\")\n",
    "\n",
    "# --- Configuration fichiers ---\n",
    "REFERENCE_CSV = os.getenv(\"REFERENCE_CSV\", \"./data/reference/reference_qa.csv\")\n",
    "OUTPUT_CSV = os.getenv(\"OUTPUT_CSV\", \"./data/results/smartrag_evaluation_results.csv\")\n",
    "\n",
    "# --- Configuration SmartRAG ---\n",
    "SMARTRAG_PROJECT_NAME = os.getenv(\"SMARTRAG_PROJECT_NAME\", \"\")\n",
    "EVALUATION_TIMERANGE = int(os.getenv(\"EVALUATION_TIMERANGE\", \"7\"))  # jours\n",
    "\n",
    "# --- Configuration évaluation ---\n",
    "MIN_CONFIDENCE_SCORE = float(os.getenv(\"MIN_CONFIDENCE_SCORE\", \"0.7\"))\n",
    "INCLUDE_FAILED_TRACES = os.getenv(\"INCLUDE_FAILED_TRACES\", \"false\").lower() == \"true\"\n",
    "\n",
    "# --- Configuration Ragas LLM (2025 models) ---\n",
    "RAGAS_LLM_PROVIDER = os.getenv(\"RAGAS_LLM_PROVIDER\", \"openai\")\n",
    "RAGAS_MODEL_NAME = os.getenv(\"RAGAS_MODEL_NAME\", \"gpt-4.1-mini\")\n",
    "\n",
    "print(\"Configuration chargée:\")\n",
    "print(f\"LANGFUSE_BASE_URL: {LANGFUSE_BASE_URL}\")\n",
    "print(f\"REFERENCE_CSV: {REFERENCE_CSV}\")\n",
    "print(f\"OUTPUT_CSV: {OUTPUT_CSV}\")\n",
    "print(f\"SMARTRAG_PROJECT_NAME: {SMARTRAG_PROJECT_NAME or 'Tous les projets'}\")\n",
    "print(f\"EVALUATION_TIMERANGE: {EVALUATION_TIMERANGE} jours\")\n",
    "print(f\"RAGAS_LLM_PROVIDER: {RAGAS_LLM_PROVIDER}\")\n",
    "print(f\"RAGAS_MODEL_NAME: {RAGAS_MODEL_NAME}\")\n",
    "\n",
    "if not LANGFUSE_PUBLIC_KEY or not LANGFUSE_SECRET_KEY:\n",
    "    raise ValueError(\"LANGFUSE_PUBLIC_KEY et LANGFUSE_SECRET_KEY sont requis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csv-structure",
   "metadata": {},
   "source": [
    "## 2) Structure du fichier CSV de référence\n",
    "\n",
    "Le fichier CSV doit contenir les colonnes suivantes :\n",
    "- `question_id` : Identifiant unique de la question\n",
    "- `question` : Texte de la question\n",
    "- `reference_answer` : Réponse de référence\n",
    "- `expected_contexts` : Contextes attendus (optionnel, séparés par '|||')\n",
    "- `category` : Catégorie de la question (optionnel)\n",
    "- `difficulty` : Niveau de difficulté (optionnel)\n",
    "\n",
    "Exemple de création d'un fichier de référence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "create-sample-csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : création d'un fichier CSV de référence (exécutez une seule fois)\n",
    "sample_data = {\n",
    "    'question_id': ['Q001', 'Q002', 'Q003'],\n",
    "    'question': [\n",
    "        'Quelle est la procédure de connexion au VPN ?',\n",
    "        'Comment réinitialiser mon mot de passe ?',\n",
    "        'Quels sont les horaires du support technique ?'\n",
    "    ],\n",
    "    'reference_answer': [\n",
    "        'Pour vous connecter au VPN, utilisez le client Cisco AnyConnect avec vos identifiants AD.',\n",
    "        'Pour réinitialiser votre mot de passe, contactez le service IT ou utilisez le portail self-service.',\n",
    "        'Le support technique est disponible du lundi au vendredi de 8h à 18h.'\n",
    "    ],\n",
    "    'expected_contexts': [\n",
    "        'Guide VPN|||Documentation réseau',\n",
    "        'Procédure mot de passe|||Guide utilisateur',\n",
    "        'Horaires support|||Contact IT'\n",
    "    ],\n",
    "    'category': ['Réseau', 'Sécurité', 'Support'],\n",
    "    'difficulty': ['Facile', 'Moyen', 'Facile']\n",
    "}\n",
    "\n",
    "# Décommentez pour créer le fichier exemple\n",
    "# sample_df = pd.DataFrame(sample_data)\n",
    "# sample_df.to_csv('reference_qa_example.csv', index=False, encoding='utf-8')\n",
    "# print(\"Fichier exemple créé : reference_qa_example.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-reference",
   "metadata": {},
   "source": [
    "## 3) Chargement du jeu de référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load-csv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERREUR : Fichier ./reference_qa_real.csv non trouvé.\n",
      "Veuillez créer votre fichier CSV de référence ou ajuster la variable REFERENCE_CSV.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './reference_qa_real.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Chargement du CSV de référence\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df_reference = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREFERENCE_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJeu de référence chargé : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_reference)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m questions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Validation des colonnes obligatoires\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG/ragas/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG/ragas/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG/ragas/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG/ragas/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG/ragas/venv/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './reference_qa_real.csv'"
     ]
    }
   ],
   "source": [
    "# Chargement du CSV de référence\n",
    "try:\n",
    "    df_reference = pd.read_csv(REFERENCE_CSV, encoding='utf-8')\n",
    "    print(f\"Jeu de référence chargé : {len(df_reference)} questions\")\n",
    "    \n",
    "    # Validation des colonnes obligatoires\n",
    "    required_cols = ['question_id', 'question', 'reference_answer']\n",
    "    missing_cols = [col for col in required_cols if col not in df_reference.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Colonnes manquantes dans le CSV : {missing_cols}\")\n",
    "    \n",
    "    # Nettoyage des données\n",
    "    df_reference['question'] = df_reference['question'].fillna('').astype(str).str.strip()\n",
    "    df_reference['reference_answer'] = df_reference['reference_answer'].fillna('').astype(str).str.strip()\n",
    "    \n",
    "    # Traitement des contextes attendus\n",
    "    if 'expected_contexts' in df_reference.columns:\n",
    "        df_reference['expected_contexts_list'] = df_reference['expected_contexts'].fillna('').apply(\n",
    "            lambda x: x.split('|||') if x else []\n",
    "        )\n",
    "    else:\n",
    "        df_reference['expected_contexts_list'] = [[] for _ in range(len(df_reference))]\n",
    "    \n",
    "    print(\"Aperçu du jeu de référence:\")\n",
    "    display(df_reference.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR : Fichier {REFERENCE_CSV} non trouvé.\")\n",
    "    print(\"Veuillez créer votre fichier CSV de référence ou ajuster la variable REFERENCE_CSV.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors du chargement du CSV : {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langfuse-connection",
   "metadata": {},
   "source": [
    "## 4) Connexion à Langfuse et récupération des traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect-langfuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# Connexion à Langfuse\n",
    "langfuse_client = Langfuse(\n",
    "    public_key=LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=LANGFUSE_SECRET_KEY,\n",
    "    host=LANGFUSE_BASE_URL\n",
    ")\n",
    "\n",
    "print(\"Connexion à Langfuse réussie\")\n",
    "\n",
    "# Définition de la période d'évaluation\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=EVALUATION_TIMERANGE)\n",
    "\n",
    "print(f\"Période d'évaluation : du {start_date.strftime('%Y-%m-%d')} au {end_date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-traces",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_smartrag_traces(client: Langfuse, start_date: datetime, end_date: datetime, \n",
    "                        project_name: Optional[str] = None) -> List[Dict]:\n",
    "    \"\"\"Récupère les traces SmartRAG depuis Langfuse\"\"\"\n",
    "    \n",
    "    traces_data = []\n",
    "    page = 1\n",
    "    limit = 100\n",
    "    \n",
    "    print(\"Récupération des traces Langfuse...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Récupération des traces avec filtres\n",
    "            traces = client.get_traces(\n",
    "                page=page,\n",
    "                limit=limit,\n",
    "                from_timestamp=start_date,\n",
    "                to_timestamp=end_date\n",
    "            )\n",
    "            \n",
    "            if not traces.data:\n",
    "                break\n",
    "                \n",
    "            print(f\"Page {page} : {len(traces.data)} traces récupérées\")\n",
    "            \n",
    "            for trace in traces.data:\n",
    "                # Filtrage par nom de projet si spécifié\n",
    "                if project_name and trace.name != project_name:\n",
    "                    continue\n",
    "                    \n",
    "                # Extraction des informations de la trace\n",
    "                trace_data = {\n",
    "                    'trace_id': trace.id,\n",
    "                    'timestamp': trace.timestamp,\n",
    "                    'name': trace.name,\n",
    "                    'input': trace.input,\n",
    "                    'output': trace.output,\n",
    "                    'metadata': trace.metadata or {},\n",
    "                    'tags': trace.tags or [],\n",
    "                    'user_id': trace.user_id,\n",
    "                    'session_id': trace.session_id,\n",
    "                    'version': trace.version,\n",
    "                    'release': trace.release,\n",
    "                    'public': trace.public\n",
    "                }\n",
    "                \n",
    "                # Récupération des spans (étapes RAG)\n",
    "                spans = client.get_trace(trace.id)\n",
    "                if spans and hasattr(spans, 'observations'):\n",
    "                    trace_data['observations'] = []\n",
    "                    for obs in spans.observations:\n",
    "                        obs_data = {\n",
    "                            'id': obs.id,\n",
    "                            'type': obs.type,\n",
    "                            'name': obs.name,\n",
    "                            'input': obs.input,\n",
    "                            'output': obs.output,\n",
    "                            'metadata': obs.metadata or {},\n",
    "                            'start_time': obs.start_time,\n",
    "                            'end_time': obs.end_time\n",
    "                        }\n",
    "                        trace_data['observations'].append(obs_data)\n",
    "                \n",
    "                traces_data.append(trace_data)\n",
    "            \n",
    "            page += 1\n",
    "            \n",
    "            # Protection contre les boucles infinies\n",
    "            if page > 100:  # Max 10000 traces\n",
    "                print(\"Limite de pages atteinte (100)\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la récupération des traces (page {page}): {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total des traces récupérées : {len(traces_data)}\")\n",
    "    return traces_data\n",
    "\n",
    "# Récupération des traces\n",
    "smartrag_traces = fetch_smartrag_traces(\n",
    "    langfuse_client, \n",
    "    start_date, \n",
    "    end_date, \n",
    "    SMARTRAG_PROJECT_NAME if SMARTRAG_PROJECT_NAME else None\n",
    ")\n",
    "\n",
    "print(f\"\\nNombre de traces SmartRAG récupérées : {len(smartrag_traces)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trace-processing",
   "metadata": {},
   "source": [
    "## 5) Traitement et correspondance des traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-traces",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rag_components(trace: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Extrait les composants RAG d'une trace Langfuse\"\"\"\n",
    "    \n",
    "    # Extraction de la question (input principal)\n",
    "    question = \"\"\n",
    "    if isinstance(trace.get('input'), str):\n",
    "        question = trace['input']\n",
    "    elif isinstance(trace.get('input'), dict):\n",
    "        question = trace['input'].get('message', trace['input'].get('question', trace['input'].get('query', '')))\n",
    "    \n",
    "    # Extraction de la réponse (output principal)\n",
    "    answer = \"\"\n",
    "    if isinstance(trace.get('output'), str):\n",
    "        answer = trace['output']\n",
    "    elif isinstance(trace.get('output'), dict):\n",
    "        answer = trace['output'].get('answer', trace['output'].get('response', trace['output'].get('text', '')))\n",
    "    \n",
    "    # Extraction des contextes depuis les observations\n",
    "    contexts = []\n",
    "    retrieval_info = {}\n",
    "    generation_info = {}\n",
    "    \n",
    "    if 'observations' in trace:\n",
    "        for obs in trace['observations']:\n",
    "            obs_name = obs.get('name', '').lower()\n",
    "            \n",
    "            # Identification des étapes de retrieval\n",
    "            if 'retriev' in obs_name or 'search' in obs_name or 'vector' in obs_name:\n",
    "                if obs.get('output'):\n",
    "                    if isinstance(obs['output'], list):\n",
    "                        contexts.extend([str(item) for item in obs['output']])\n",
    "                    elif isinstance(obs['output'], dict):\n",
    "                        if 'documents' in obs['output']:\n",
    "                            contexts.extend([str(doc) for doc in obs['output']['documents']])\n",
    "                        elif 'results' in obs['output']:\n",
    "                            contexts.extend([str(res) for res in obs['output']['results']])\n",
    "                    else:\n",
    "                        contexts.append(str(obs['output']))\n",
    "                \n",
    "                retrieval_info.update({\n",
    "                    'retrieval_duration': obs.get('end_time', 0) - obs.get('start_time', 0) if obs.get('end_time') and obs.get('start_time') else None,\n",
    "                    'retrieval_metadata': obs.get('metadata', {})\n",
    "                })\n",
    "            \n",
    "            # Identification des étapes de génération\n",
    "            elif 'generat' in obs_name or 'llm' in obs_name or 'model' in obs_name:\n",
    "                generation_info.update({\n",
    "                    'generation_duration': obs.get('end_time', 0) - obs.get('start_time', 0) if obs.get('end_time') and obs.get('start_time') else None,\n",
    "                    'generation_metadata': obs.get('metadata', {}),\n",
    "                    'model_name': obs.get('metadata', {}).get('model', 'unknown')\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        'trace_id': trace['trace_id'],\n",
    "        'timestamp': trace['timestamp'],\n",
    "        'question': question.strip(),\n",
    "        'answer': answer.strip(),\n",
    "        'contexts': contexts,\n",
    "        'session_id': trace.get('session_id'),\n",
    "        'user_id': trace.get('user_id'),\n",
    "        'metadata': trace.get('metadata', {}),\n",
    "        'tags': trace.get('tags', []),\n",
    "        **retrieval_info,\n",
    "        **generation_info\n",
    "    }\n",
    "\n",
    "# Traitement des traces\n",
    "print(\"Traitement des traces SmartRAG...\")\n",
    "processed_traces = []\n",
    "\n",
    "for trace in smartrag_traces:\n",
    "    try:\n",
    "        processed = extract_rag_components(trace)\n",
    "        if processed['question'] and processed['answer']:  # On ne garde que les traces complètes\n",
    "            processed_traces.append(processed)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de la trace {trace.get('trace_id', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Traces traitées avec succès : {len(processed_traces)}\")\n",
    "\n",
    "# Conversion en DataFrame\n",
    "df_traces = pd.DataFrame(processed_traces)\n",
    "if len(df_traces) > 0:\n",
    "    print(\"\\nAperçu des traces traitées:\")\n",
    "    display(df_traces[['trace_id', 'timestamp', 'question', 'answer']].head())\n",
    "else:\n",
    "    print(\"\\nAucune trace valide trouvée.\")\n",
    "    print(\"Vérifiez :\")\n",
    "    print(\"- Que SmartRAG envoie bien les traces à Langfuse\")\n",
    "    print(\"- La période d'évaluation (EVALUATION_TIMERANGE)\")\n",
    "    print(\"- Le nom du projet SmartRAG (SMARTRAG_PROJECT_NAME)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matching-section",
   "metadata": {},
   "source": [
    "## 6) Correspondance questions de référence ↔ traces SmartRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "match-traces",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calcule la similarité entre deux textes\"\"\"\n",
    "    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "def match_traces_to_reference(df_traces: pd.DataFrame, df_reference: pd.DataFrame, \n",
    "                            similarity_threshold: float = 0.8) -> pd.DataFrame:\n",
    "    \"\"\"Fait correspondre les traces aux questions de référence\"\"\"\n",
    "    \n",
    "    matched_data = []\n",
    "    \n",
    "    for _, ref_row in df_reference.iterrows():\n",
    "        ref_question = ref_row['question']\n",
    "        best_match = None\n",
    "        best_similarity = 0\n",
    "        \n",
    "        # Recherche de la meilleure correspondance\n",
    "        for _, trace_row in df_traces.iterrows():\n",
    "            similarity = calculate_similarity(ref_question, trace_row['question'])\n",
    "            \n",
    "            if similarity > best_similarity and similarity >= similarity_threshold:\n",
    "                best_similarity = similarity\n",
    "                best_match = trace_row\n",
    "        \n",
    "        if best_match is not None:\n",
    "            matched_row = {\n",
    "                # Données de référence\n",
    "                'question_id': ref_row['question_id'],\n",
    "                'reference_question': ref_row['question'],\n",
    "                'reference_answer': ref_row['reference_answer'],\n",
    "                'expected_contexts': ref_row.get('expected_contexts_list', []),\n",
    "                'category': ref_row.get('category', ''),\n",
    "                'difficulty': ref_row.get('difficulty', ''),\n",
    "                \n",
    "                # Données de la trace\n",
    "                'trace_id': best_match['trace_id'],\n",
    "                'actual_question': best_match['question'],\n",
    "                'actual_answer': best_match['answer'],\n",
    "                'retrieved_contexts': best_match['contexts'],\n",
    "                'question_similarity': best_similarity,\n",
    "                \n",
    "                # Métadonnées de la trace\n",
    "                'timestamp': best_match['timestamp'],\n",
    "                'session_id': best_match.get('session_id'),\n",
    "                'user_id': best_match.get('user_id'),\n",
    "                'retrieval_duration': best_match.get('retrieval_duration'),\n",
    "                'generation_duration': best_match.get('generation_duration'),\n",
    "                'model_name': best_match.get('model_name', 'unknown'),\n",
    "                'trace_metadata': best_match.get('metadata', {}),\n",
    "                'trace_tags': best_match.get('tags', [])\n",
    "            }\n",
    "            matched_data.append(matched_row)\n",
    "        else:\n",
    "            print(f\"Pas de correspondance trouvée pour la question: {ref_row['question_id']} (seuil: {similarity_threshold})\")\n",
    "    \n",
    "    return pd.DataFrame(matched_data)\n",
    "\n",
    "# Correspondance des traces\n",
    "if len(df_traces) > 0:\n",
    "    print(\"Correspondance des traces aux questions de référence...\")\n",
    "    df_matched = match_traces_to_reference(df_traces, df_reference, similarity_threshold=0.7)\n",
    "    \n",
    "    print(f\"\\nCorrespondances trouvées : {len(df_matched)}/{len(df_reference)}\")\n",
    "    \n",
    "    if len(df_matched) > 0:\n",
    "        print(f\"Similarité moyenne des questions : {df_matched['question_similarity'].mean():.3f}\")\n",
    "        display(df_matched[['question_id', 'question_similarity', 'reference_question', 'actual_question']].head())\n",
    "    else:\n",
    "        print(\"Aucune correspondance trouvée. Essayez de réduire le seuil de similarité ou vérifiez les questions.\")\n",
    "else:\n",
    "    print(\"Impossible de faire les correspondances : aucune trace disponible.\")\n",
    "    df_matched = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ragas-eval",
   "metadata": {},
   "source": [
    "## 7) Évaluation Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ragas-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall, context_entities_recall, noise_sensitivity\n",
    "\n",
    "if len(df_matched) > 0:\n",
    "    print(f\"Évaluation Ragas de {len(df_matched)} questions...\")\n",
    "    \n",
    "    # Préparation des données pour Ragas\n",
    "    eval_data = df_matched.copy()\n",
    "    \n",
    "    # Vérification et nettoyage des contextes\n",
    "    eval_data['contexts_cleaned'] = eval_data['retrieved_contexts'].apply(\n",
    "        lambda x: [str(ctx).strip() for ctx in x] if isinstance(x, list) and x else [\"Aucun contexte\"]\n",
    "    )\n",
    "    \n",
    "    # Mapping des colonnes pour Ragas\n",
    "    column_map = {\n",
    "        \"question\": \"reference_question\",\n",
    "        \"answer\": \"actual_answer\",\n",
    "        \"contexts\": \"contexts_cleaned\",\n",
    "        \"ground_truth\": \"reference_answer\"\n",
    "    }\n",
    "    \n",
    "    # Sélection des métriques\n",
    "    metrics = [\n",
    "        faithfulness,           # Fidélité au contexte\n",
    "        answer_relevancy,       # Pertinence de la réponse\n",
    "        context_precision,      # Précision du contexte\n",
    "        context_recall,         # Rappel du contexte\n",
    "        context_entities_recall, # Rappel des entités\n",
    "        noise_sensitivity       # Sensibilité au bruit\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Évaluation Ragas\n",
    "        ragas_results = evaluate(eval_data, metrics=metrics, column_map=column_map)\n",
    "        \n",
    "        print(\"\\n=== RÉSULTATS RAGAS ===\")\n",
    "        print(\"\\nScores moyens:\")\n",
    "        for metric, score in ragas_results.scores.items():\n",
    "            print(f\"  {metric}: {score:.4f}\")\n",
    "        \n",
    "        # Fusion des résultats avec les données matchées\n",
    "        df_final = df_matched.merge(ragas_results.results, left_index=True, right_index=True, how='left')\n",
    "        \n",
    "        print(f\"\\nÉvaluation terminée pour {len(df_final)} questions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation Ragas: {e}\")\n",
    "        df_final = df_matched.copy()\n",
    "        # Ajout de colonnes vides pour les métriques\n",
    "        for metric in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', \n",
    "                      'context_entities_recall', 'noise_sensitivity']:\n",
    "            df_final[metric] = None\n",
    "else:\n",
    "    print(\"Aucune donnée à évaluer.\")\n",
    "    df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-section",
   "metadata": {},
   "source": [
    "## 8) Analyse et visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if len(df_final) > 0 and 'faithfulness' in df_final.columns:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Graphique 1 : Scores moyens par métrique\n",
    "    plt.subplot(2, 3, 1)\n",
    "    metrics_cols = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', \n",
    "                   'context_entities_recall', 'noise_sensitivity']\n",
    "    \n",
    "    available_metrics = [col for col in metrics_cols if col in df_final.columns and df_final[col].notna().any()]\n",
    "    \n",
    "    if available_metrics:\n",
    "        means = [df_final[col].mean() for col in available_metrics]\n",
    "        plt.bar(range(len(available_metrics)), means, color='skyblue')\n",
    "        plt.xticks(range(len(available_metrics)), available_metrics, rotation=45, ha='right')\n",
    "        plt.title('Scores Ragas moyens')\n",
    "        plt.ylabel('Score')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        for i, v in enumerate(means):\n",
    "            plt.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Graphique 2 : Distribution des scores de fidélité\n",
    "    plt.subplot(2, 3, 2)\n",
    "    if 'faithfulness' in df_final.columns and df_final['faithfulness'].notna().any():\n",
    "        plt.hist(df_final['faithfulness'].dropna(), bins=20, color='lightcoral', alpha=0.7)\n",
    "        plt.title('Distribution - Faithfulness')\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Fréquence')\n",
    "    \n",
    "    # Graphique 3 : Scores par catégorie (si disponible)\n",
    "    plt.subplot(2, 3, 3)\n",
    "    if 'category' in df_final.columns and df_final['category'].notna().any() and 'faithfulness' in df_final.columns:\n",
    "        category_scores = df_final.groupby('category')['faithfulness'].mean().sort_values(ascending=False)\n",
    "        plt.bar(range(len(category_scores)), category_scores.values, color='lightgreen')\n",
    "        plt.xticks(range(len(category_scores)), category_scores.index, rotation=45, ha='right')\n",
    "        plt.title('Faithfulness par catégorie')\n",
    "        plt.ylabel('Score moyen')\n",
    "    \n",
    "    # Graphique 4 : Corrélation similarité de question vs. qualité de réponse\n",
    "    plt.subplot(2, 3, 4)\n",
    "    if 'question_similarity' in df_final.columns and 'answer_relevancy' in df_final.columns:\n",
    "        valid_data = df_final[df_final['answer_relevancy'].notna() & df_final['question_similarity'].notna()]\n",
    "        if len(valid_data) > 0:\n",
    "            plt.scatter(valid_data['question_similarity'], valid_data['answer_relevancy'], alpha=0.6)\n",
    "            plt.xlabel('Similarité question')\n",
    "            plt.ylabel('Answer Relevancy')\n",
    "            plt.title('Similarité vs. Pertinence')\n",
    "    \n",
    "    # Graphique 5 : Performance par modèle\n",
    "    plt.subplot(2, 3, 5)\n",
    "    if 'model_name' in df_final.columns and 'faithfulness' in df_final.columns:\n",
    "        model_scores = df_final.groupby('model_name')['faithfulness'].mean().sort_values(ascending=False)\n",
    "        if len(model_scores) > 1:\n",
    "            plt.bar(range(len(model_scores)), model_scores.values, color='gold')\n",
    "            plt.xticks(range(len(model_scores)), model_scores.index, rotation=45, ha='right')\n",
    "            plt.title('Faithfulness par modèle')\n",
    "            plt.ylabel('Score moyen')\n",
    "    \n",
    "    # Graphique 6 : Évolution temporelle (si plusieurs jours)\n",
    "    plt.subplot(2, 3, 6)\n",
    "    if 'timestamp' in df_final.columns and len(df_final) > 1:\n",
    "        df_final['date'] = pd.to_datetime(df_final['timestamp']).dt.date\n",
    "        daily_scores = df_final.groupby('date')['faithfulness'].mean()\n",
    "        if len(daily_scores) > 1:\n",
    "            plt.plot(daily_scores.index, daily_scores.values, marker='o', color='purple')\n",
    "            plt.title('Évolution Faithfulness')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Score moyen')\n",
    "            plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques détaillées\n",
    "    print(\"\\n=== STATISTIQUES DÉTAILLÉES ===\")\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        values = df_final[metric].dropna()\n",
    "        if len(values) > 0:\n",
    "            print(f\"\\n{metric.upper()}:\")\n",
    "            print(f\"  Moyenne: {values.mean():.4f}\")\n",
    "            print(f\"  Médiane: {values.median():.4f}\")\n",
    "            print(f\"  Écart-type: {values.std():.4f}\")\n",
    "            print(f\"  Min: {values.min():.4f}\")\n",
    "            print(f\"  Max: {values.max():.4f}\")\n",
    "            \n",
    "            # Questions avec les plus mauvais scores\n",
    "            worst_idx = values.idxmin()\n",
    "            print(f\"  Pire score: Question {df_final.loc[worst_idx, 'question_id']} ({values.min():.4f})\")\n",
    "            \n",
    "            # Questions avec les meilleurs scores\n",
    "            best_idx = values.idxmax()\n",
    "            print(f\"  Meilleur score: Question {df_final.loc[best_idx, 'question_id']} ({values.max():.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\"Aucune donnée disponible pour les visualisations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## 9) Export des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_final) > 0:\n",
    "    # Préparation des colonnes d'export\n",
    "    export_columns = [\n",
    "        'question_id', 'category', 'difficulty', 'question_similarity',\n",
    "        'reference_question', 'actual_question', 'reference_answer', 'actual_answer',\n",
    "        'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall',\n",
    "        'context_entities_recall', 'noise_sensitivity',\n",
    "        'trace_id', 'timestamp', 'session_id', 'user_id', 'model_name',\n",
    "        'retrieval_duration', 'generation_duration'\n",
    "    ]\n",
    "    \n",
    "    # Filtrage des colonnes existantes\n",
    "    available_columns = [col for col in export_columns if col in df_final.columns]\n",
    "    \n",
    "    # Export CSV\n",
    "    df_export = df_final[available_columns].copy()\n",
    "    df_export.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Résultats exportés vers : {OUTPUT_CSV}\")\n",
    "    print(f\"Colonnes exportées : {len(available_columns)}\")\n",
    "    print(f\"Lignes exportées : {len(df_export)}\")\n",
    "    \n",
    "    # Export JSON pour analyse approfondie\n",
    "    json_output = OUTPUT_CSV.replace('.csv', '_detailed.json')\n",
    "    \n",
    "    # Préparation des données JSON avec plus de détails\n",
    "    json_data = {\n",
    "        'evaluation_metadata': {\n",
    "            'evaluation_date': datetime.now().isoformat(),\n",
    "            'evaluation_period': {\n",
    "                'start': start_date.isoformat(),\n",
    "                'end': end_date.isoformat()\n",
    "            },\n",
    "            'total_reference_questions': len(df_reference),\n",
    "            'matched_questions': len(df_final),\n",
    "            'smartrag_project': SMARTRAG_PROJECT_NAME or 'All projects',\n",
    "            'langfuse_url': LANGFUSE_BASE_URL\n",
    "        },\n",
    "        'summary_metrics': {},\n",
    "        'detailed_results': df_final.to_dict('records')\n",
    "    }\n",
    "    \n",
    "    # Calcul des métriques de résumé\n",
    "    ragas_metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', \n",
    "                    'context_entities_recall', 'noise_sensitivity']\n",
    "    \n",
    "    for metric in ragas_metrics:\n",
    "        if metric in df_final.columns and df_final[metric].notna().any():\n",
    "            values = df_final[metric].dropna()\n",
    "            json_data['summary_metrics'][metric] = {\n",
    "                'mean': float(values.mean()),\n",
    "                'median': float(values.median()),\n",
    "                'std': float(values.std()),\n",
    "                'min': float(values.min()),\n",
    "                'max': float(values.max()),\n",
    "                'count': int(len(values))\n",
    "            }\n",
    "    \n",
    "    # Sauvegarde JSON\n",
    "    with open(json_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Analyse détaillée exportée vers : {json_output}\")\n",
    "    \n",
    "    # Aperçu des résultats\n",
    "    print(\"\\n=== APERÇU DES RÉSULTATS EXPORTÉS ===\")\n",
    "    display(df_export.head())\n",
    "\n",
    "else:\n",
    "    print(\"Aucune donnée à exporter.\")\n",
    "    print(\"\\nPoints de vérification :\")\n",
    "    print(\"1. Vérifiez que SmartRAG envoie bien les traces à Langfuse\")\n",
    "    print(\"2. Ajustez la période d'évaluation (EVALUATION_TIMERANGE)\")\n",
    "    print(\"3. Vérifiez le nom du projet SmartRAG (SMARTRAG_PROJECT_NAME)\")\n",
    "    print(\"4. Vérifiez que les questions dans votre CSV correspondent aux questions posées à SmartRAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## 10) Conclusions et prochaines étapes\n",
    "\n",
    "Ce notebook vous permet de :\n",
    "1. **Récupérer** automatiquement les traces SmartRAG depuis Langfuse\n",
    "2. **Comparer** les réponses générées avec vos données de référence\n",
    "3. **Évaluer** la qualité avec les métriques Ragas\n",
    "4. **Analyser** les performances par catégorie, modèle, période\n",
    "5. **Exporter** les résultats pour analyse approfondie\n",
    "\n",
    "### Prochaines étapes suggérées :\n",
    "\n",
    "- **Itération sur les configurations SmartRAG** : Modifiez les paramètres de SmartRAG (chunking, embedding, reranking) et relancez l'évaluation\n",
    "- **Comparaison A/B** : Utilisez les sessions Langfuse pour comparer différentes versions de votre pipeline\n",
    "- **Monitoring continu** : Automatisez ce notebook pour un monitoring régulier de la qualité\n",
    "- **Analyse des échecs** : Identifiez les questions avec les plus mauvais scores pour améliorer le système\n",
    "- **Extension du jeu de test** : Enrichissez votre CSV de référence avec plus de questions représentatives\n",
    "\n",
    "### Optimisation des performances :\n",
    "- Ajustez le seuil de similarité pour la correspondance des questions\n",
    "- Utilisez les tags Langfuse pour filtrer par version ou configuration\n",
    "- Exploitez les métadonnées pour analyser les performances par type de document ou utilisateur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
