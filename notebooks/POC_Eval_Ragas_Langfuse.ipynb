{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836f6245",
   "metadata": {},
   "source": [
    "# POC — Évaluation RAG avec Ragas + Langfuse\n",
    "Ce notebook charge le **jeu d’or** (questions/réponses de référence), exécute votre pipeline (placeholders fournis),\n",
    "calcule les métriques Ragas, exporte les scores dans l’Excel et peut tracer dans **Langfuse**.\n",
    "\n",
    "**Prérequis** : Python 3.10+, accès au modèle (Bedrock/OpenAI/endpoint interne), et packages listés ci‑dessous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbded2",
   "metadata": {},
   "source": [
    "## 0) Installation des dépendances\n",
    "_Exécutez une fois. Adaptez selon votre environnement (proxy, versions)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2d093",
   "metadata": {},
   "outputs": [],
   "source": "# Si nécessaire, décommentez pour installer\n# %pip install --upgrade pip\n# %pip install ragas==0.2.6 langfuse==2.46.7 pydantic>=2.7.0 pandas openpyxl xlsxwriter matplotlib python-dotenv requests"
  },
  {
   "cell_type": "markdown",
   "id": "d123485e",
   "metadata": {},
   "source": "## 1) Configuration\nRenseignez vos variables d'environnement.\n- `EVAL_INPUT_XLSX` : chemin vers le classeur Excel (celui fourni en sortie du template)\n- `EVAL_OUTPUT_XLSX` : chemin de sortie (le même fichier ou un nouveau)\n- `MODEL_PROVIDER` : `bedrock` | `openai` | `custom` | `ollama`\n- `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, `LANGFUSE_BASE_URL` si vous utilisez Langfuse\n- `BATCH_SIZE` : taille du lot pour l'inférence\n\n**Variables Ollama :**\n- `OLLAMA_BASE_URL` : URL de votre instance Ollama (défaut: http://localhost:11434)\n- `OLLAMA_MODEL` : nom du modèle (ex: llama2, mistral, codellama)\n- `OLLAMA_TIMEOUT` : timeout en secondes\n- `OLLAMA_TEMPERATURE`, `OLLAMA_TOP_P`, `OLLAMA_TOP_K` : paramètres de génération"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f68951",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# --- Paramètres principaux ---\nEVAL_INPUT_XLSX = os.getenv(\"EVAL_INPUT_XLSX\", \"/mnt/data/Template_gold_POCEval.xlsx\")\nEVAL_OUTPUT_XLSX = os.getenv(\"EVAL_OUTPUT_XLSX\", \"/mnt/data/Resultats_POCEval.xlsx\")\nMODEL_PROVIDER = os.getenv(\"MODEL_PROVIDER\", \"ollama\")  # bedrock | openai | custom | ollama\nBATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"16\"))\n\n# --- Langfuse ---\nLANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"\")\nLANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\", \"\")\nLANGFUSE_BASE_URL  = os.getenv(\"LANGFUSE_BASE_URL\", \"https://cloud.langfuse.com\")\n\n# --- Ollama ---\nOLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\nOLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama2\")\nOLLAMA_TIMEOUT = int(os.getenv(\"OLLAMA_TIMEOUT\", \"120\"))\nOLLAMA_NUM_PREDICT = int(os.getenv(\"OLLAMA_NUM_PREDICT\", \"512\"))\nOLLAMA_TEMPERATURE = float(os.getenv(\"OLLAMA_TEMPERATURE\", \"0.7\"))\nOLLAMA_TOP_P = float(os.getenv(\"OLLAMA_TOP_P\", \"0.9\"))\nOLLAMA_TOP_K = int(os.getenv(\"OLLAMA_TOP_K\", \"40\"))\n\nprint(\"EVAL_INPUT_XLSX:\", EVAL_INPUT_XLSX)\nprint(\"EVAL_OUTPUT_XLSX:\", EVAL_OUTPUT_XLSX)\nprint(\"MODEL_PROVIDER:\", MODEL_PROVIDER)\nif MODEL_PROVIDER == \"ollama\":\n    print(\"OLLAMA_BASE_URL:\", OLLAMA_BASE_URL)\n    print(\"OLLAMA_MODEL:\", OLLAMA_MODEL)"
  },
  {
   "cell_type": "markdown",
   "id": "b68344c3",
   "metadata": {},
   "source": [
    "## 2) Chargement du jeu d’or\n",
    "On lit l’onglet **JEU_OR** et les **SOURCES**. On valide quelques contraintes simples pour éviter les surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ba530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_or = pd.read_excel(EVAL_INPUT_XLSX, sheet_name=\"JEU_OR\")\n",
    "df_sources = pd.read_excel(EVAL_INPUT_XLSX, sheet_name=\"SOURCES\")\n",
    "\n",
    "required_cols = [\"id\",\"question\",\"reponse_reference\"]\n",
    "missing = [c for c in required_cols if c not in df_or.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes dans JEU_OR: {missing}\")\n",
    "\n",
    "# Normalisation\n",
    "df_or[\"question\"] = df_or[\"question\"].fillna(\"\").astype(str).str.strip()\n",
    "df_or[\"reponse_reference\"] = df_or[\"reponse_reference\"].fillna(\"\").astype(str).str.strip()\n",
    "df_or[\"contexte_attendu\"] = df_or.get(\"contexte_attendu\", \"\").fillna(\"\").astype(str)\n",
    "print(\"Taille du jeu d’or:\", len(df_or))\n",
    "df_or.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffc58b",
   "metadata": {},
   "source": "## 3) Connecteurs modèle (placeholders)\nQuatre exemples : **Bedrock**, **OpenAI**, **Custom endpoint**, **Ollama**.\nRemplacez les implémentations pour brancher votre vrai pipeline RAG (retrieval + génération)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e120660",
   "metadata": {},
   "outputs": [],
   "source": "from typing import List, Dict, Any\nimport requests\nimport time\n\n# --- PLACEHOLDER: récupère des contexts en se basant sur 'contexte_attendu' s'il existe.\n# Remplacez par un vrai retriever (Vector DB + reranker, etc.).\ndef dummy_retrieve_contexts(row) -> List[str]:\n    ctx = row.get(\"contexte_attendu\", \"\")\n    if isinstance(ctx, str) and ctx.strip():\n        return [ctx]\n    return []\n\n# --- Connecteur Ollama\ndef call_ollama(prompt: str, max_retries: int = 3) -> Dict[str, Any]:\n    \"\"\"Appel à l'API Ollama avec retry et gestion d'erreur\"\"\"\n    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n    payload = {\n        \"model\": OLLAMA_MODEL,\n        \"prompt\": prompt,\n        \"stream\": False,\n        \"options\": {\n            \"temperature\": OLLAMA_TEMPERATURE,\n            \"top_p\": OLLAMA_TOP_P,\n            \"top_k\": OLLAMA_TOP_K,\n            \"num_predict\": OLLAMA_NUM_PREDICT\n        }\n    }\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.post(url, json=payload, timeout=OLLAMA_TIMEOUT)\n            response.raise_for_status()\n            result = response.json()\n            return {\n                \"response\": result.get(\"response\", \"\"),\n                \"total_duration\": result.get(\"total_duration\", 0),\n                \"load_duration\": result.get(\"load_duration\", 0),\n                \"prompt_eval_count\": result.get(\"prompt_eval_count\", 0),\n                \"eval_count\": result.get(\"eval_count\", 0)\n            }\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                print(f\"Erreur Ollama après {max_retries} tentatives: {e}\")\n                return {\n                    \"response\": \"Erreur: Impossible de contacter Ollama\",\n                    \"total_duration\": 0,\n                    \"load_duration\": 0,\n                    \"prompt_eval_count\": 0,\n                    \"eval_count\": 0\n                }\n            time.sleep(2 ** attempt)  # Backoff exponentiel\n    \n# --- PLACEHOLDER: génération (remplacez par vos appels Bedrock/OpenAI/SmartRAG/Ollama)\ndef generate_answer(question: str, contexts: List[str]) -> Dict[str, Any]:\n    \"\"\"Génère une réponse selon le provider configuré\"\"\"\n    \n    if MODEL_PROVIDER == \"ollama\":\n        # Construction du prompt RAG\n        context_text = \"\\n\\n\".join(contexts) if contexts else \"Aucun contexte disponible.\"\n        prompt = f\"\"\"Contexte:\n{context_text}\n\nQuestion: {question}\n\nRéponds à la question en te basant uniquement sur le contexte fourni. Si le contexte ne contient pas l'information nécessaire, indique-le clairement.\n\nRéponse:\"\"\"\n        \n        result = call_ollama(prompt)\n        return {\n            \"answer\": result[\"response\"].strip(),\n            \"tokens_input\": result[\"prompt_eval_count\"],\n            \"tokens_output\": result[\"eval_count\"],\n            \"duration_ms\": result[\"total_duration\"] / 1000000,  # Conversion ns -> ms\n            \"model_version\": f\"{OLLAMA_MODEL}-ollama\"\n        }\n    \n    else:\n        # Fallback pour les autres providers (placeholder)\n        base = \" \".join(contexts)[:800]\n        if not base:\n            base = \"Je ne dispose pas du contexte interne. Veuillez consulter la procédure de référence.\"\n        return {\n            \"answer\": f\"Réponse (démo) basée sur le contexte: {base}\",\n            \"tokens_input\": None,\n            \"tokens_output\": None,\n            \"duration_ms\": 0,\n            \"model_version\": \"demo-0.1\"\n        }"
  },
  {
   "cell_type": "markdown",
   "id": "9f4b2691",
   "metadata": {},
   "source": [
    "## 4) Inférence par batch\n",
    "On produit : `reponse_modele` et `contexts_utilises` pour chaque question.\n",
    "On enregistre également des métadonnées (version, config, latence si vous l’avez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744dbd50",
   "metadata": {},
   "outputs": [],
   "source": "from time import perf_counter\nresults: List[Dict[str, Any]] = []\n\nprint(f\"Traitement de {len(df_or)} questions avec le provider: {MODEL_PROVIDER}\")\nif MODEL_PROVIDER == \"ollama\":\n    print(f\"Modèle Ollama: {OLLAMA_MODEL}\")\n\nfor idx, row in df_or.iterrows():\n    qid = row[\"id\"]\n    q = row[\"question\"]\n    gt = row[\"reponse_reference\"]\n    contexts = dummy_retrieve_contexts(row)\n\n    print(f\"Traitement question {idx+1}/{len(df_or)}: {qid}\")\n    \n    t0 = perf_counter()\n    generation_result = generate_answer(q, contexts)\n    total_latency_ms = (perf_counter() - t0) * 1000.0\n    \n    # Extraction des informations selon le format retourné\n    if isinstance(generation_result, dict):\n        answer = generation_result.get(\"answer\", \"\")\n        tokens_input = generation_result.get(\"tokens_input\")\n        tokens_output = generation_result.get(\"tokens_output\")\n        model_duration_ms = generation_result.get(\"duration_ms\", 0)\n        model_version = generation_result.get(\"model_version\", \"unknown\")\n    else:\n        # Rétrocompatibilité si generate_answer retourne une string\n        answer = str(generation_result)\n        tokens_input = None\n        tokens_output = None\n        model_duration_ms = 0\n        model_version = \"demo-0.1\"\n\n    results.append({\n        \"id\": qid,\n        \"question\": q,\n        \"reponse_reference\": gt,\n        \"reponse_modele\": answer,\n        \"contexts_utilises\": contexts,\n        \"latence_ms\": total_latency_ms,\n        \"latence_modele_ms\": model_duration_ms,\n        \"tokens_entres\": tokens_input,\n        \"tokens_sorties\": tokens_output,\n        \"cout_estime\": None,  # À calculer selon vos tarifs\n        \"version_modele\": model_version,\n        \"config_retrieval\": json.dumps({\"provider\": MODEL_PROVIDER, \"k\": len(contexts)}),\n    })\n\ndf_pred = pd.DataFrame(results)\nprint(f\"\\nTaille des résultats: {len(df_pred)}\")\nprint(f\"Latence moyenne: {df_pred['latence_ms'].mean():.2f}ms\")\nif MODEL_PROVIDER == \"ollama\":\n    print(f\"Tokens moyens entrée: {df_pred['tokens_entres'].mean():.1f}\")\n    print(f\"Tokens moyens sortie: {df_pred['tokens_sorties'].mean():.1f}\")\ndf_pred.head(2)"
  },
  {
   "cell_type": "markdown",
   "id": "4d11b2d0",
   "metadata": {},
   "source": [
    "## 5) Évaluation avec Ragas\n",
    "On calcule les principales métriques Ragas et on agrège une synthèse.\n",
    "Le `column_map` permet d’aligner nos noms de colonnes au format attendu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall, context_entities_recall, noise_sensitivity\n",
    "\n",
    "# Préparation du dataset au format Ragas\n",
    "# Ragas attend typiquement: question(s), answer(s), contexts (list[str]), ground_truth(s)\n",
    "# On mappe nos colonnes:\n",
    "column_map = {\n",
    "    \"question\": \"question\",\n",
    "    \"answer\": \"reponse_modele\",\n",
    "    \"contexts\": \"contexts_utilises\",\n",
    "    \"ground_truth\": \"reponse_reference\",\n",
    "}\n",
    "\n",
    "# Ragas requiert des listes/series aux bons types\n",
    "eval_df = df_pred.copy()\n",
    "# Contexte doit être une liste de str\n",
    "eval_df[\"contexts_utilises\"] = eval_df[\"contexts_utilises\"].apply(lambda x: x if isinstance(x, list) else ([] if pd.isna(x) else [str(x)]))\n",
    "\n",
    "metrics = [faithfulness, answer_relevancy, context_precision, context_recall, context_entities_recall, noise_sensitivity]\n",
    "ragas_res = evaluate(eval_df, metrics=metrics, column_map=column_map)\n",
    "\n",
    "print(\"Scores par item:\")\n",
    "display(ragas_res.results)\n",
    "\n",
    "print(\"\\nMoyennes globales:\")\n",
    "display(ragas_res.scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6fede",
   "metadata": {},
   "source": [
    "## 6) Export des résultats vers Excel\n",
    "Écrit les réponses, contextes et scores dans l’onglet `SORTIE_EVALUATIONS`. Si le fichier n’existe pas, il sera créé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cde64e",
   "metadata": {},
   "outputs": [],
   "source": "# Fusion des scores item-level avec df_pred\nscored = df_pred.merge(ragas_res.results, left_index=True, right_index=True, how=\"left\")\n\n# Colonnes de sortie (ajout de latence_modele_ms)\nexport_cols = [\n    \"id\",\"question\",\"reponse_reference\",\"reponse_modele\",\"contexts_utilises\",\n    \"faithfulness\",\"answer_relevancy\",\"context_precision\",\"context_recall\",\n    \"context_entities_recall\",\"noise_sensitivity\",\"latence_ms\",\"latence_modele_ms\",\n    \"tokens_entres\",\"tokens_sorties\",\"cout_estime\",\"version_modele\",\"config_retrieval\"\n]\nfor c in export_cols:\n    if c not in scored.columns:\n        scored[c] = None\n\n# Écriture\nwith pd.ExcelWriter(EVAL_OUTPUT_XLSX, engine=\"xlsxwriter\") as writer:\n    # Copier les feuilles source si besoin\n    try:\n        df_or.to_excel(writer, index=False, sheet_name=\"JEU_OR\")\n        df_sources.to_excel(writer, index=False, sheet_name=\"SOURCES\")\n    except Exception:\n        pass\n    # Écrire les scores\n    scored.to_excel(writer, index=False, sheet_name=\"SORTIE_EVALUATIONS\")\n\nprint(\"Export terminé:\", EVAL_OUTPUT_XLSX)"
  },
  {
   "cell_type": "markdown",
   "id": "fa993c9f",
   "metadata": {},
   "source": [
    "## 7) (Optionnel) Traçage dans Langfuse\n",
    "Cette section enregistre les traces au fil de l’eau. Décommentez et adaptez si vous utilisez Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87559966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langfuse import Langfuse\n",
    "# if LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY:\n",
    "#     lf = Langfuse(public_key=LANGFUSE_PUBLIC_KEY, secret_key=LANGFUSE_SECRET_KEY, base_url=LANGFUSE_BASE_URL)\n",
    "#     for _, r in scored.iterrows():\n",
    "#         trace = lf.trace(name=\"poc-rag-eval\", input=r[\"question\"], output=r[\"reponse_modele\"], metadata={\n",
    "#             \"id\": r[\"id\"],\n",
    "#             \"version_modele\": r[\"version_modele\"],\n",
    "#             \"config_retrieval\": r[\"config_retrieval\"],\n",
    "#             \"scores\": {\n",
    "#                 \"faithfulness\": r.get(\"faithfulness\"),\n",
    "#                 \"answer_relevancy\": r.get(\"answer_relevancy\"),\n",
    "#                 \"context_precision\": r.get(\"context_precision\"),\n",
    "#                 \"context_recall\": r.get(\"context_recall\"),\n",
    "#                 \"context_entities_recall\": r.get(\"context_entities_recall\"),\n",
    "#                 \"noise_sensitivity\": r.get(\"noise_sensitivity\"),\n",
    "#             }\n",
    "#         })\n",
    "#     print(\"Traces envoyées à Langfuse.\")\n",
    "# else:\n",
    "#     print(\"Langfuse non configuré (variables d’environnement manquantes).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2d4e7",
   "metadata": {},
   "source": [
    "## 8) Synthèse & graphiques rapides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "means = ragas_res.scores.to_dict()\n",
    "labels = list(means.keys())\n",
    "values = [means[k] for k in labels]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, values)\n",
    "plt.title(\"Scores moyens (Ragas)\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7b90b",
   "metadata": {},
   "source": "## 9) Prochaines étapes\n- Remplacer les **placeholders** de retrieval/génération par votre pipeline réel (SmartRAG/Bedrock/etc.).\n- Ajouter des variantes (chunking, top‑k, reranker, prompt) et **réexécuter** pour comparer.\n- Utiliser Langfuse **Datasets & Runs** pour benchmarker plusieurs configs en parallèle.\n- **Ollama** : Tester différents modèles locaux (llama2, mistral, codellama) en changeant `OLLAMA_MODEL`.\n- Optimiser les paramètres Ollama (`temperature`, `top_p`, `top_k`) selon votre cas d'usage."
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}