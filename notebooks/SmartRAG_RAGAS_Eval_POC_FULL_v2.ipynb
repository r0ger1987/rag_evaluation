{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5a8d6d",
   "metadata": {},
   "source": [
    "\n",
    "# üß™ SmartRAG ‚Äî √âvaluation RAG (POC M√©tier) avec **RAGAS** ‚Äî Version POC FULL\n",
    "\n",
    "Ce notebook met en ≈ìuvre les **m√©triques d'√©valuation** demand√©es et des **analyses avanc√©es** adapt√©es √† votre fichier CSV contenant :  \n",
    "- **R√©f√©rences m√©tier** : `question`, `reference_answer`, `sharepoint_document`  \n",
    "- **Sorties du syst√®me RAG** : `ragas_question`, `ragas_answer`, `ragas_contexts`, `ragas_ground_truth`  \n",
    "\n",
    "### M√©triques RAGAS calcul√©es\n",
    "1. üéØ **Faithfulness (Fid√©lit√©)** ‚Äî coh√©rence factuelle *r√©ponse ‚Üî contextes*  \n",
    "2. ‚úÖ **Answer Correctness (Correction)** ‚Äî *r√©ponse ‚Üî v√©rit√© m√©tier (reference_answer)*  \n",
    "3. üí¨ **Relevancy (Pertinence)** ‚Äî *r√©ponse ‚Üî question* (auto-d√©tection `response_relevancy`/`answer_relevancy`)  \n",
    "4. üéØ **Context Precision (Pr√©cision)** ‚Äî pertinence des contextes r√©cup√©r√©s  \n",
    "5. üìö **Context Recall (Rappel)** ‚Äî compl√©tude des contextes r√©cup√©r√©s  \n",
    "\n",
    "### Analyses additionnelles (POC)\n",
    "- **Mesures documentaires** (√† partir de `sharepoint_document` et `ragas_ground_truth`) : *doc-precision/recall/F1*  \n",
    "- **Diagnostics d√©taill√©s** (tailles, Jaccard coverage, #contexts, etc.)  \n",
    "- **Visualisations** et **explications** (histos, scatters, corr√©lations, effets de n_contexts, per-doc)  \n",
    "- **Recommandations automatiques** & **Plan d‚Äôexp√©riences** (pr√©processing PDF, embeddings, chunking, recherche hybride, reranking, prompts de g√©n√©ration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8446957",
   "metadata": {},
   "source": [
    "## 0) Installation & v√©rifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4268d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.13/site-packages (8.1.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ce36d4e9474e54ae895b0a40a467c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='widgets OK', max=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm notebook OK ‚úÖ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a451074c0b4d60877580f55549c9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test tqdm (widgets):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Installer\n",
    "!pip install -U ipywidgets tqdm\n",
    "\n",
    "# 2) Pas besoin de \"jupyter nbextension ...\" en JupyterLab (normal que la commande n‚Äôexiste pas)\n",
    "# 3) IMPORTANT : red√©marre le kernel ici (Kernel > Restart)\n",
    "\n",
    "# 4) Apr√®s red√©marrage :\n",
    "import ipywidgets as W\n",
    "from IPython.display import display\n",
    "try:\n",
    "    display(W.IntProgress(min=0, max=1, description=\"widgets OK\"))\n",
    "    from tqdm.notebook import tqdm\n",
    "    print(\"tqdm notebook OK ‚úÖ\")\n",
    "except Exception as e:\n",
    "    from tqdm.auto import tqdm\n",
    "    print(\"Fallback tqdm auto ‚ö†Ô∏è ->\", e)\n",
    "\n",
    "# (test)\n",
    "_ = [x for x in tqdm(range(50), desc=\"Test tqdm (widgets)\", leave=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa725810",
   "metadata": {},
   "source": [
    "## 1) Configuration ‚Äî LLM & Chemins de fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f8813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Fournisseur LLM: \"openai\" | \"claude\" | \"gemini\" | \"ollama\"\n",
    "RAGAS_LLM_PROVIDER = os.getenv(\"RAGAS_LLM_PROVIDER\", \"openai\").lower()\n",
    "\n",
    "OPENAI_MODEL  = os.getenv(\"OPENAI_MODEL\",  \"gpt-4o-mini\")\n",
    "CLAUDE_MODEL  = os.getenv(\"CLAUDE_MODEL\",  \"claude-3-5-sonnet-20240620\")\n",
    "GEMINI_MODEL  = os.getenv(\"GEMINI_MODEL\",  \"gemini-1.5-pro\")\n",
    "OLLAMA_MODEL  = os.getenv(\"OLLAMA_MODEL\",  \"llama3.1:8b\")\n",
    "\n",
    "# Cl√©s d'API attendues dans l'environnement\n",
    "os.environ[\"OPENAI_API_KEY\"]     = \"your-openai-api-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"]  = \"...\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"]     = \"...\"\n",
    "\n",
    "# Donn√©es\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\", \"reference_qa_manuel_template.csv\")\n",
    "\n",
    "# Sorties\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Provider:\", RAGAS_LLM_PROVIDER)\n",
    "print(\"Data path:\", DATA_PATH)\n",
    "print(\"Output dir:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63329606",
   "metadata": {},
   "source": [
    "## 2) Chargement du CSV & aper√ßu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f31cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    alt = '../data/reference/reference_qa_manuel_template.csv'\n",
    "    if os.path.exists(alt):\n",
    "        DATA_PATH = alt\n",
    "        print(f\"INFO: DATA_PATH introuvable, utilisation de {alt}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"CSV introuvable: {DATA_PATH}\")\n",
    "\n",
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", raw_df.shape)\n",
    "display(raw_df.head(5))\n",
    "print(\"Colonnes:\", list(raw_df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b77182",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Normalisation ‚Üí Cl√©s attendues par RAGAS\n",
    "\n",
    "**Mapping POC ‚Üí RAGAS**  \n",
    "- `question` (r√©f√©rence m√©tier) **&** `ragas_question` (question r√©ellement pos√©e au syst√®me)  \n",
    "  ‚Üí On prend **`ragas_question`** si pr√©sent, sinon `question`  \n",
    "- `ragas_answer` ‚Üí **`answer`**  \n",
    "- `ragas_contexts` ‚Üí **`contexts`** (List[str])  \n",
    "- `reference_answer` ‚Üí **`ground_truth`** (texte pour `answer_correctness`)  \n",
    "- `sharepoint_document` ‚Üí **`reference_docs`** (List[str]) ‚Äî pour analyses documentaires  \n",
    "- `ragas_ground_truth` ‚Üí **`cited_docs`** (List[str]) ‚Äî documents cit√©s par la r√©ponse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast, math\n",
    "\n",
    "df = raw_df.copy()\n",
    "\n",
    "# Colonnes attendues c√¥t√© POC (r√©f√©rence + RAG)\n",
    "COLS_REQUIRED = [\n",
    "    \"question\", \"reference_answer\", \"sharepoint_document\",\n",
    "    \"ragas_question\", \"ragas_answer\", \"ragas_contexts\", \"ragas_ground_truth\"\n",
    "]\n",
    "missing = [c for c in COLS_REQUIRED if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"‚ö†Ô∏è Colonnes manquantes (ok si volontairement absentes) :\", missing)\n",
    "\n",
    "def to_list_generic(x):\n",
    "    \"Convertit une cellule en List[str] : listes s√©rialis√©es, s√©parateurs (|||, ;;, \\n, ,), simple cha√Æne.\"\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(xx).strip() for xx in x if str(xx).strip()]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith('[') and s.endswith(']')) or (s.startswith('(') and s.endswith(')')):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return [str(xx).strip() for xx in parsed if str(xx).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        for sep in [\"|||\", \"¬ß¬ß\", \";;\", \"##\", \"\\n\", \",\"]:\n",
    "            if sep in s:\n",
    "                parts = [p.strip() for p in s.split(sep)]\n",
    "                return [p for p in parts if p]\n",
    "        return [s]\n",
    "    return [str(x).strip()]\n",
    "\n",
    "# question ‚Üí ragas_question si pr√©sent, sinon question\n",
    "if \"ragas_question\" in df.columns and df[\"ragas_question\"].notna().any():\n",
    "    q_series = df[\"ragas_question\"].fillna(df.get(\"question\",\"\"))\n",
    "else:\n",
    "    q_series = df.get(\"question\",\"\")\n",
    "\n",
    "# answer\n",
    "a_series = df.get(\"ragas_answer\",\"\").fillna(\"\")\n",
    "\n",
    "# contexts\n",
    "ctx_series_raw = df.get(\"ragas_contexts\",\"\").fillna(\"\")\n",
    "contexts = [to_list_generic(v) for v in ctx_series_raw.tolist()]\n",
    "\n",
    "# ground_truth (texte m√©tier)\n",
    "gt_series = df.get(\"reference_answer\",\"\").fillna(\"\")\n",
    "\n",
    "# reference_docs (liste)\n",
    "ref_docs_series_raw = df.get(\"sharepoint_document\",\"\").fillna(\"\")\n",
    "reference_docs = [to_list_generic(v) for v in ref_docs_series_raw.tolist()]\n",
    "\n",
    "# cited_docs (liste) ‚Äî documents cit√©s par la r√©ponse\n",
    "cited_docs_series_raw = df.get(\"ragas_ground_truth\",\"\").fillna(\"\")\n",
    "cited_docs = [to_list_generic(v) for v in cited_docs_series_raw.tolist()]\n",
    "\n",
    "dataset_dict = {\n",
    "    \"question\": q_series.astype(str).tolist(),\n",
    "    \"answer\": a_series.astype(str).tolist(),\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": gt_series.astype(str).tolist(),\n",
    "}\n",
    "\n",
    "print(\"Exemple ‚Äî question:\", dataset_dict[\"question\"][0] if len(dataset_dict[\"question\"]) else \"n/a\")\n",
    "print(\"Exemple ‚Äî answer:\", dataset_dict[\"answer\"][0] if len(dataset_dict[\"answer\"]) else \"n/a\")\n",
    "print(\"Exemple ‚Äî contexts[0]:\", dataset_dict[\"contexts\"][0][:2] if len(dataset_dict[\"contexts\"]) else \"n/a\")\n",
    "print(\"Exemple ‚Äî ground_truth:\", dataset_dict[\"ground_truth\"][0] if len(dataset_dict[\"ground_truth\"]) else \"n/a\")\n",
    "\n",
    "# Infos documentaires (hors-RAGAS)\n",
    "aux_docs = {\n",
    "    \"reference_docs\": reference_docs,\n",
    "    \"cited_docs\": cited_docs,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b210b",
   "metadata": {},
   "source": [
    "## 4) Construction du Dataset (HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset as HFDataset\n",
    "hf_dataset = HFDataset.from_dict(dataset_dict)\n",
    "hf_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b7082",
   "metadata": {},
   "source": [
    "## 5) LLM compatible RAGAS (Wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "def build_llm(provider: str):\n",
    "    provider = provider.lower().strip()\n",
    "    if provider == \"openai\":\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        lc = ChatOpenAI(model=OPENAI_MODEL, temperature=0)\n",
    "        return LangchainLLMWrapper(lc)\n",
    "    elif provider == \"claude\":\n",
    "        from langchain_anthropic import ChatAnthropic\n",
    "        lc = ChatAnthropic(model=CLAUDE_MODEL, temperature=0)\n",
    "        return LangchainLLMWrapper(lc)\n",
    "    elif provider == \"gemini\":\n",
    "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "        lc = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0)\n",
    "        return LangchainLLMWrapper(lc)\n",
    "    elif provider == \"ollama\":\n",
    "        try:\n",
    "            from langchain_community.chat_models import ChatOllama\n",
    "            lc = ChatOllama(model=OLLAMA_MODEL)\n",
    "        except Exception:\n",
    "            from langchain_community.llms import Ollama\n",
    "            lc = Ollama(model=OLLAMA_MODEL)\n",
    "        return LangchainLLMWrapper(lc)\n",
    "    else:\n",
    "        raise ValueError(f\"Provider non support√©: {provider}\")\n",
    "\n",
    "llm = build_llm(RAGAS_LLM_PROVIDER)\n",
    "print(\"‚úÖ LLM pr√™t pour RAGAS:\", type(llm).__name__, \"| provider:\", RAGAS_LLM_PROVIDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667de56",
   "metadata": {},
   "source": [
    "## 6) M√©triques RAGAS (auto-d√©tection de *Relevancy*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b08f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_precision, context_recall\n",
    "\n",
    "# Auto-d√©tecte le bon alias selon la version de ragas\n",
    "try:\n",
    "    from ragas.metrics import response_relevancy as _relevancy_metric\n",
    "    RELEVANCY_NAME = \"response_relevancy\"\n",
    "except Exception:\n",
    "    from ragas.metrics import answer_relevancy as _relevancy_metric\n",
    "    RELEVANCY_NAME = \"answer_relevancy\"\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,           # 1. Fid√©lit√© (r√©ponse vs contexts)\n",
    "    answer_correctness,     # 2. Correction (r√©ponse vs ground_truth)\n",
    "    _relevancy_metric,      # 3. Pertinence (r√©ponse vs question)\n",
    "    context_precision,      # 4. Pr√©cision des contextes r√©cup√©r√©s\n",
    "    context_recall,         # 5. Rappel des contextes r√©cup√©r√©s\n",
    "]\n",
    "\n",
    "print(\"M√©trique de pertinence retenue:\", RELEVANCY_NAME)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c5f97",
   "metadata": {},
   "source": [
    "## 7) Ex√©cution de l‚Äô√©valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas import evaluate\n",
    "import os\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=hf_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm,\n",
    "    raise_exceptions=False,\n",
    "    show_progress=False,  # √©vite d√©pendance ipywidgets\n",
    ")\n",
    "\n",
    "print(\"‚úÖ √âvaluation termin√©e.\")\n",
    "df_results = result.to_pandas()\n",
    "display(df_results.head(10))\n",
    "\n",
    "csv_out = os.path.join(OUTPUT_DIR, \"ragas_raw_results.csv\")\n",
    "df_results.to_csv(csv_out, index=False, encoding=\"utf-8\")\n",
    "print(\"R√©sultats enregistr√©s ->\", csv_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac6ee3",
   "metadata": {},
   "source": [
    "## 8) Synth√®se des scores (0‚Äì1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de591b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "rel_col = \"response_relevancy\" if \"response_relevancy\" in df_results.columns else (\n",
    "    \"answer_relevancy\" if \"answer_relevancy\" in df_results.columns else None\n",
    ")\n",
    "\n",
    "wanted_cols = [\"faithfulness\", \"answer_correctness\", \"context_precision\", \"context_recall\"]\n",
    "if rel_col:\n",
    "    wanted_cols.insert(2, rel_col)\n",
    "\n",
    "present = [c for c in wanted_cols if c in df_results.columns]\n",
    "summary = {c: float(np.nanmean(df_results[c])) for c in present}\n",
    "\n",
    "print(\"üìä Scores moyens :\")\n",
    "for k, v in summary.items():\n",
    "    print(f\" - {k}: {v:.3f}\")\n",
    "\n",
    "summary_out = os.path.join(OUTPUT_DIR, \"ragas_summary.json\")\n",
    "with open(summary_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"generated_at\": datetime.now().isoformat(),\n",
    "        \"provider\": RAGAS_LLM_PROVIDER,\n",
    "        \"model\": {\n",
    "            \"openai\": OPENAI_MODEL,\n",
    "            \"claude\": CLAUDE_MODEL,\n",
    "            \"gemini\": GEMINI_MODEL,\n",
    "            \"ollama\": OLLAMA_MODEL,\n",
    "        }.get(RAGAS_LLM_PROVIDER, \"n/a\"),\n",
    "        \"scores\": summary,\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Synth√®se enregistr√©e ->\", summary_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefd16b",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Diagnostics enrichis & features d√©riv√©es\n",
    "\n",
    "On ajoute des signaux utiles au **debug RAG** :  \n",
    "- `n_contexts`, `avg_context_len`, `total_context_len`  \n",
    "- `answer_len`, `question_len`, `gt_len`  \n",
    "- `context_coverage_jaccard` : similitude *ground_truth ‚Üî contexts_concat*  \n",
    "- `answer_coverage_jaccard` : similitude *ground_truth ‚Üî answer*  \n",
    "- **Doc-level**¬†: *doc_precision/recall/F1* √† partir de `sharepoint_document` (r√©f√©rence) et `ragas_ground_truth` (cit√©s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3069c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, re, os\n",
    "\n",
    "def wc(s: str) -> int:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    return len(re.findall(r\"\\w+\", s))\n",
    "\n",
    "def jaccard_words(a: str, b: str) -> float:\n",
    "    A = set([w.lower() for w in re.findall(r\"\\w+\", a or \"\")])\n",
    "    B = set([w.lower() for w in re.findall(r\"\\w+\", b or \"\")])\n",
    "    if not A and not B:\n",
    "        return 0.0\n",
    "    return len(A & B) / max(1, len(A | B))\n",
    "\n",
    "def f1(p, r):\n",
    "    if p + r == 0:\n",
    "        return 0.0\n",
    "    return 2*p*r/(p+r)\n",
    "\n",
    "# Concat contexts\n",
    "contexts_concat = [\"\\n\".join(c) if isinstance(c, (list, tuple)) else str(c) for c in dataset_dict[\"contexts\"]]\n",
    "\n",
    "# Doc-level metrics\n",
    "doc_precisions, doc_recalls, doc_f1s = [], [], []\n",
    "for i in range(len(aux_docs[\"reference_docs\"])):\n",
    "    refs = set([x.lower() for x in aux_docs[\"reference_docs\"][i]])\n",
    "    cits = set([x.lower() for x in aux_docs[\"cited_docs\"][i]])\n",
    "    inter = refs & cits\n",
    "    p = len(inter)/max(1, len(cits)) if len(cits)>0 else 0.0\n",
    "    r = len(inter)/max(1, len(refs)) if len(refs)>0 else 0.0\n",
    "    doc_precisions.append(p); doc_recalls.append(r); doc_f1s.append(f1(p,r))\n",
    "\n",
    "enriched = pd.DataFrame({\n",
    "    \"question\": dataset_dict[\"question\"],\n",
    "    \"answer\": dataset_dict[\"answer\"],\n",
    "    \"ground_truth\": dataset_dict[\"ground_truth\"],\n",
    "    \"contexts_concat\": contexts_concat,\n",
    "    \"n_contexts\": [len(c) if isinstance(c, (list, tuple)) else 0 for c in dataset_dict[\"contexts\"]],\n",
    "    \"avg_context_len\": [np.mean([wc(x) for x in c]) if isinstance(c, (list, tuple)) and len(c)>0 else 0 for c in dataset_dict[\"contexts\"]],\n",
    "    \"total_context_len\": [np.sum([wc(x) for x in c]) if isinstance(c, (list, tuple)) else 0 for c in dataset_dict[\"contexts\"]],\n",
    "    \"answer_len\": [wc(a) for a in dataset_dict[\"answer\"]],\n",
    "    \"question_len\": [wc(q) for q in dataset_dict[\"question\"]],\n",
    "    \"gt_len\": [wc(g) for g in dataset_dict[\"ground_truth\"]],\n",
    "    \"context_coverage_jaccard\": [jaccard_words(dataset_dict[\"ground_truth\"][i], contexts_concat[i]) for i in range(len(contexts_concat))],\n",
    "    \"answer_coverage_jaccard\": [jaccard_words(dataset_dict[\"ground_truth\"][i], dataset_dict[\"answer\"][i]) for i in range(len(contexts_concat))],\n",
    "    \"doc_precision\": doc_precisions,\n",
    "    \"doc_recall\": doc_recalls,\n",
    "    \"doc_f1\": doc_f1s,\n",
    "})\n",
    "\n",
    "df_all = pd.concat([enriched, df_results.reset_index(drop=True)], axis=1)\n",
    "\n",
    "enriched_out = os.path.join(OUTPUT_DIR, \"ragas_results_enriched.csv\")\n",
    "df_all.to_csv(enriched_out, index=False, encoding=\"utf-8\")\n",
    "print(\"Enrichi ->\", enriched_out)\n",
    "display(df_all.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb749d",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Visualisations ‚Äî Distributions (histogrammes)\n",
    "\n",
    "Ces graphiques montrent **l'√©talement des scores**¬†: s'ils sont concentr√©s vers 1 ‚Üí bon; vers 0 ‚Üí √† travailler ; tr√®s dispers√©s ‚Üí comportement instable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "rel_col = \"response_relevancy\" if \"response_relevancy\" in df_all.columns else (\"answer_relevancy\" if \"answer_relevancy\" in df_all.columns else None)\n",
    "score_cols = [\"faithfulness\",\"answer_correctness\",\"context_precision\",\"context_recall\",\"doc_precision\",\"doc_recall\",\"doc_f1\"]\n",
    "if rel_col:\n",
    "    score_cols.insert(2, rel_col)\n",
    "\n",
    "for col in score_cols:\n",
    "    if col in df_all.columns:\n",
    "        plt.figure()\n",
    "        df_all[col].dropna().plot(kind=\"hist\", bins=10, title=f\"Distribution ‚Äî {col}\")\n",
    "        plt.xlabel(col); plt.ylabel(\"Fr√©quence\")\n",
    "        plt.tight_layout()\n",
    "        outp = os.path.join(OUTPUT_DIR, f\"hist_{col}.png\")\n",
    "        plt.savefig(outp); print(\"Saved:\", outp)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701671f3",
   "metadata": {},
   "source": [
    "\n",
    "**Lecture :**  \n",
    "- Si `doc_recall` est faible ‚Üí le syst√®me **ne cite pas** suffisamment les bons documents (voir *pr√©processing*, *retrieval*, *reranking*).  \n",
    "- Si `faithfulness` est basse avec `context_precision` haute ‚Üí **hallucinations de g√©n√©ration** (ajuster prompts/LLM).  \n",
    "- Si `context_precision` et `context_recall` sont bas ‚Üí **retrieval √† optimiser** (hybride, top-k, embeddings, chunking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a99b7",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Focus erreurs ‚Äî Top-K items les plus faibles\n",
    "\n",
    "Pour **prioriser** les investigations, voici les pires cas par m√©trique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def topk_worst(col, k=15):\n",
    "    if col not in df_all.columns:\n",
    "        return pd.DataFrame()\n",
    "    sub = df_all[[\"question\",\"answer\",\"ground_truth\", col]].copy()\n",
    "    sub = sub.sort_values(col, ascending=True).head(k)\n",
    "    return sub\n",
    "\n",
    "print(\"### Pires 'answer_correctness'\")\n",
    "display(topk_worst(\"answer_correctness\"))\n",
    "\n",
    "print(\"\\n### Pires 'faithfulness'\")\n",
    "display(topk_worst(\"faithfulness\"))\n",
    "\n",
    "if rel_col:\n",
    "    print(f\"\\n### Pires '{rel_col}'\")\n",
    "    display(topk_worst(rel_col))\n",
    "\n",
    "print(\"\\n### Pires 'doc_recall' (documents manquants)\")\n",
    "display(topk_worst(\"doc_recall\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f007f7e",
   "metadata": {},
   "source": [
    "\n",
    "**Lecture :**  \n",
    "- Regardez **les contextes** associ√©s √† ces cas : souvent des probl√®mes de d√©coupe (chunk) ou d'extraction PDF.  \n",
    "- Comparez `sharepoint_document` vs `ragas_ground_truth` pour voir **quels documents attendus manquent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a645a3",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Scatter diagnostics ‚Äî Relations entre m√©triques\n",
    "\n",
    "Nuages de points pour **d√©tecter des patterns**¬†: trop de contextes ? faible recouvrement ? corr√©lations inattendues ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbceb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scatter_xy(x, y):\n",
    "    import matplotlib.pyplot as plt\n",
    "    if x in df_all.columns and y in df_all.columns:\n",
    "        plt.figure()\n",
    "        plt.scatter(df_all[x], df_all[y])\n",
    "        plt.xlabel(x); plt.ylabel(y)\n",
    "        plt.title(f\"{x} vs {y}\")\n",
    "        plt.tight_layout()\n",
    "        outp = os.path.join(OUTPUT_DIR, f\"scatter_{x}_vs_{y}.png\")\n",
    "        plt.savefig(outp); print(\"Saved:\", outp)\n",
    "        plt.show()\n",
    "\n",
    "scatter_xy(\"faithfulness\", \"answer_correctness\")\n",
    "if rel_col: scatter_xy(rel_col, \"answer_correctness\")\n",
    "scatter_xy(\"context_precision\", \"context_recall\")\n",
    "scatter_xy(\"n_contexts\", \"answer_correctness\")\n",
    "scatter_xy(\"avg_context_len\", \"answer_correctness\")\n",
    "scatter_xy(\"context_coverage_jaccard\", \"answer_correctness\")\n",
    "scatter_xy(\"answer_coverage_jaccard\", \"answer_correctness\")\n",
    "scatter_xy(\"doc_recall\", \"answer_correctness\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a118b5",
   "metadata": {},
   "source": [
    "\n",
    "**Lecture :**  \n",
    "- `n_contexts` ‚ÜòÔ∏é `answer_correctness` ‚Üí **trop de bruit** : r√©duire top‚Äëk ou **reranker**.  \n",
    "- `context_coverage_jaccard` ‚ÜóÔ∏é `answer_correctness` ‚Üí meilleurs contextes **mieux align√©s** √† la v√©rit√© m√©tier.  \n",
    "- `doc_recall` ‚ÜóÔ∏é `answer_correctness` ‚Üí citer les **bons documents** aide la justesse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69afbd5",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Corr√©lations ‚Äî Matrice (Pearson)\n",
    "\n",
    "Utile pour **prioriser les leviers** : qu‚Äôest‚Äëce qui corr√®le le plus avec `answer_correctness` ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ac4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = df_all.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr = df_all[num_cols].corr()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(corr, aspect='auto')\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "plt.colorbar()\n",
    "plt.title(\"Matrice de corr√©lations (Pearson)\")\n",
    "plt.tight_layout()\n",
    "png = os.path.join(OUTPUT_DIR, \"corr_matrix.png\")\n",
    "plt.savefig(png); print(\"Saved:\", png)\n",
    "plt.show()\n",
    "\n",
    "if \"answer_correctness\" in corr.columns:\n",
    "    display(corr.sort_values(by=\"answer_correctness\", ascending=False)[[\"answer_correctness\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfd848",
   "metadata": {},
   "source": [
    "\n",
    "**Lecture :** Les variables en haut de la liste sont des **candidats prioritaires** √† optimiser (ex. `doc_recall`, `context_coverage_jaccard`, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768084e",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Effet du nombre de contextes ‚Äî Agr√©gations par *bins*\n",
    "\n",
    "Observe l‚Äô√©volution des m√©triques par classes de `n_contexts` (oriente **top‚Äëk**, **reranking**, **hybride**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0380d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "def agg_by_bins(col, bins=(0,1,2,3,4,6,10,999)):\n",
    "    if col not in df_all.columns:\n",
    "        return pd.DataFrame()\n",
    "    b = pd.cut(df_all[col], bins=bins, right=True)\n",
    "    cols = [\"faithfulness\",\"answer_correctness\",\"context_precision\",\"context_recall\",\"doc_recall\"]\n",
    "    if rel_col and rel_col in df_all.columns: cols.append(rel_col)\n",
    "    agg = df_all.groupby(b)[cols].mean(numeric_only=True)\n",
    "    return agg.reset_index()\n",
    "\n",
    "agg_ctx = agg_by_bins(\"n_contexts\")\n",
    "display(agg_ctx)\n",
    "\n",
    "for c in [c for c in agg_ctx.columns if c != \"n_contexts\"]:\n",
    "    plt.figure()\n",
    "    x = agg_ctx.iloc[:,0].astype(str)\n",
    "    y = agg_ctx[c]\n",
    "    plt.bar(x, y)\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.ylabel(c)\n",
    "    plt.title(f\"{c} moyen par bin de n_contexts\")\n",
    "    plt.tight_layout()\n",
    "    outp = os.path.join(OUTPUT_DIR, f\"bar_{c}_by_ncontexts_bins.png\")\n",
    "    plt.savefig(outp); print(\"Saved:\", outp)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782febfd",
   "metadata": {},
   "source": [
    "\n",
    "**Lecture :** Si `answer_correctness` baisse apr√®s un certain **top‚Äëk**, c‚Äôest un signal pour **r√©duire** les passages envoy√©s en g√©n√©ration et/ou **renforcer le reranking**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431e162",
   "metadata": {},
   "source": [
    "\n",
    "## 15) Analyse par document/source (`sharepoint_document`)\n",
    "\n",
    "Identifier les **documents qui posent probl√®me** (extraction PDF, structuration, obsolescence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d446de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "src_col = \"sharepoint_document\" if \"sharepoint_document\" in raw_df.columns else None\n",
    "\n",
    "if src_col:\n",
    "    metric_cols = [\"faithfulness\",\"answer_correctness\",\"context_precision\",\"context_recall\",\"doc_recall\",\"doc_precision\",\"doc_f1\"]\n",
    "    if rel_col and rel_col in df_all.columns:\n",
    "        metric_cols.insert(2, rel_col)\n",
    "    per_src = pd.concat([raw_df[[src_col]], df_all[metric_cols]], axis=1)\n",
    "    agg_src = per_src.groupby(src_col).mean(numeric_only=True).sort_values(\"answer_correctness\", ascending=False)\n",
    "    display(agg_src.head(10))\n",
    "\n",
    "    topk = agg_src.head(10)\n",
    "    if len(topk) > 0:\n",
    "        plt.figure()\n",
    "        plt.bar(topk.index.astype(str), topk[\"answer_correctness\"])\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.ylabel(\"answer_correctness\")\n",
    "        plt.title(\"Top documents ‚Äî answer_correctness moyen\")\n",
    "        plt.tight_layout()\n",
    "        outp = os.path.join(OUTPUT_DIR, \"bar_top_docs_answer_correctness.png\")\n",
    "        plt.savefig(outp); print(\"Saved:\", outp)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Aucune colonne 'sharepoint_document' d√©tect√©e.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b250aabf",
   "metadata": {},
   "source": [
    "\n",
    "**Lecture :** Les documents en bas du classement n√©cessitent souvent un **pr√©processing PDF** plus robuste (OCR, nettoyage headers/footers, gestion des tableaux, extraction fid√®le du texte).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e0f93",
   "metadata": {},
   "source": [
    "\n",
    "## 16) Recommandations automatiques (guid√©es par les m√©triques)\n",
    "\n",
    "Pistes d‚Äôoptimisation √† partir des signaux : **pr√©processing PDF, embeddings, chunking, retrieval (hybride), reranking, prompts**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd537fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "reco = []\n",
    "\n",
    "def add(msg):\n",
    "    print(\"‚Ä¢\", msg); reco.append(\"‚Ä¢ \" + msg)\n",
    "\n",
    "mean = df_all.mean(numeric_only=True).to_dict()\n",
    "m = lambda k, d=mean: d.get(k, None)\n",
    "\n",
    "if (m(\"faithfulness\") or 0) < 0.6:\n",
    "    add(\"Fid√©lit√© basse : citer explicitement les passages (verbatim), r√©duire temp√©rature (0), contraindre la r√©ponse (format, r√©f√©rences).\")\n",
    "\n",
    "if (m(\"answer_correctness\") or 0) < 0.6 and (m(\"context_recall\") or 0) >= 0.6:\n",
    "    add(\"R√©ponses incorrectes malgr√© un rappel correct : renforcer les **prompts de g√©n√©ration** (extraction stricte) et la **post-v√©rification** (self-check).\")\n",
    "\n",
    "if (m(\"context_recall\") or 0) < 0.6:\n",
    "    add(\"Rappel faible : augmenter **top-k**, utiliser la **recherche hybride** (BM25 + vecteur), am√©liorer **chunking** et **pr√©processing PDF** (OCR, headers/footers).\")\n",
    "\n",
    "if (m(\"context_precision\") or 0) < 0.6:\n",
    "    add(\"Pr√©cision faible : ajouter un **reranker** (cross-encoder/LLM), baisser **top-k** avant g√©n√©ration, filtrer par **m√©tadonn√©es**.\")\n",
    "\n",
    "if \"n_contexts\" in df_all and \"answer_correctness\" in df_all and df_all[\"n_contexts\"].corr(df_all[\"answer_correctness\"]) < -0.15:\n",
    "    add(\"Trop de contextes nuit √† la correction : **r√©duire top-k** et/ou reranker plus agressivement.\")\n",
    "\n",
    "if (m(\"doc_recall\") or 0) < 0.6:\n",
    "    add(\"Faible rappel documentaire : **aligner le nommage des documents**, am√©liorer les **citations automatiques** et le **pr√©processing PDF** (extraction fid√®le).\")\n",
    "\n",
    "if (m(\"context_coverage_jaccard\") or 0) < 0.4:\n",
    "    add(\"Faible recouvrement GT‚ÜîContexts : v√©rifier extraction PDF/OCR, **embeddings** adapt√©s au FR/domaine, **chunking** par sections/titres.\")\n",
    "\n",
    "# Sauvegarde\n",
    "txt_out = os.path.join(OUTPUT_DIR, \"auto_recommendations.txt\")\n",
    "with open(txt_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Recommandations automatiques\\n\\n\")\n",
    "    for r in reco: f.write(r + \"\\n\")\n",
    "print(\"Recommandations ->\", txt_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36118364",
   "metadata": {},
   "source": [
    "\n",
    "## 17) Plan d‚Äôexp√©riences (A/B & grille)\n",
    "\n",
    "It√©rations recommand√©es :  \n",
    "- **Pr√©processing** *(pdfminer/pymupdf + tesseract OCR, nettoyage headers/footers, normalisation espaces)*  \n",
    "- **Embeddings** *(mod√®les FR/domaine, dimension, normalisation)*  \n",
    "- **Chunking** *(d√©coupe s√©mantique par titres/sections ou hierachique)*  \n",
    "- **Recherche** *(hybride BM25+vecteur, pond√©ration, filtres m√©tadonn√©es)*  \n",
    "- **Reranking** *(cross-encoder, LLM-as-a-reranker, top‚Äëk)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd388809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, os\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocess\": [\n",
    "        {\"ocr\": False, \"clean_headers\": True, \"normalize_ws\": True},\n",
    "        {\"ocr\": True,  \"clean_headers\": True, \"normalize_ws\": True},\n",
    "    ],\n",
    "    \"embedding\": [\n",
    "        {\"provider\":\"openai\",\"model\":\"text-embedding-3-large\"},\n",
    "        {\"provider\":\"openai\",\"model\":\"text-embedding-3-small\"},\n",
    "        {\"provider\":\"nomic\",\"model\":\"nomic-embed-text\"},\n",
    "    ],\n",
    "    \"chunking\": [\n",
    "        {\"method\":\"fixed\",\"size\":512,\"overlap\":64},\n",
    "        {\"method\":\"fixed\",\"size\":800,\"overlap\":100},\n",
    "        {\"method\":\"semantic\",\"size\":\"auto\",\"overlap\":64},\n",
    "    ],\n",
    "    \"retrieval\": [\n",
    "        {\"type\":\"vector\",\"top_k\":8},\n",
    "        {\"type\":\"hybrid\",\"bm25_weight\":0.4,\"top_k\":8},\n",
    "        {\"type\":\"hybrid\",\"bm25_weight\":0.6,\"top_k\":12},\n",
    "    ],\n",
    "    \"rerank\": [\n",
    "        {\"enabled\": False},\n",
    "        {\"enabled\": True, \"model\":\"cross-encoder/ms-marco-MiniLM-L-6-v2\", \"top_k\":5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(json.dumps(param_grid, indent=2, ensure_ascii=False))\n",
    "grid_out = os.path.join(OUTPUT_DIR, \"experiment_plan.json\")\n",
    "with open(grid_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(param_grid, f, ensure_ascii=False, indent=2)\n",
    "print(\"Plan d'exp√©riences ->\", grid_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00c2a0",
   "metadata": {},
   "source": [
    "\n",
    "## 18) Checklist d‚Äôoptimisation RAG (rapide)\n",
    "\n",
    "- **PDF** : OCR si scans; enlever headers/footers/num√©ros; g√©rer tableaux; normaliser espaces/casse.  \n",
    "- **Embeddings** : FR/domaine; normalisation; taille vecteur suffisante; re‚Äëindexer apr√®s changement.  \n",
    "- **Chunking** : 512‚Äì800 tokens + overlap 64‚Äì100; d√©coupe par sections/titres; isoler tableaux/code.  \n",
    "- **Index** : m√©tadonn√©es (titre, section, date, doc_id) pour filtrage.  \n",
    "- **Retrieval** : tester **hybride** (BM25+vecteur), pond√©ration; top‚Äëk √©quilibr√©.  \n",
    "- **Reranking** : cross‚Äëencoder/LLM; r√©duire √† 3‚Äì5 passages de haute qualit√©.  \n",
    "- **G√©n√©ration** : prompts de citation stricte (verbatim + doc_id); temp√©rature 0; JSON strict; self‚Äëcheck.  \n",
    "- **√âvaluation** : r√©f√©rentiel √† jour; seuils GO/NO‚ÄëGO; journaliser la config de run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646f7e2-c4ed-4884-9e29-382f3ed454e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
