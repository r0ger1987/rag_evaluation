# SmartRAG Evaluation Framework with Ragas

Un framework complet pour Ã©valuer les performances de SmartRAG en utilisant les mÃ©triques Ragas via l'intÃ©gration Langfuse.

## ğŸ“ Structure du Projet

```
ragas/
â”œâ”€â”€ src/                      # Code source
â”‚   â”œâ”€â”€ evaluation/          # Modules d'Ã©valuation
â”‚   â”‚   â””â”€â”€ smartrag_evaluator.py
â”‚   â”œâ”€â”€ utils/              # Utilitaires
â”‚   â””â”€â”€ config/             # Configuration
â”œâ”€â”€ data/                    # DonnÃ©es
â”‚   â”œâ”€â”€ reference/          # Questions/rÃ©ponses de rÃ©fÃ©rence
â”‚   â”‚   â””â”€â”€ reference_qa.csv
â”‚   â””â”€â”€ results/            # RÃ©sultats d'Ã©valuation
â”œâ”€â”€ notebooks/              # Jupyter notebooks
â”œâ”€â”€ config/                 # Fichiers de configuration
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ tests/                  # Tests unitaires
â”œâ”€â”€ evaluate.py             # Script principal
â”œâ”€â”€ .env                    # Variables d'environnement (Ã  crÃ©er)
â”œâ”€â”€ .env.template           # Template des variables
â””â”€â”€ requirements.txt        # DÃ©pendances Python
```

## ğŸš€ Installation

### 1. Cloner le projet
```bash
cd /home/roger/RAG/ragas
```

### 2. CrÃ©er un environnement virtuel
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 4. Configuration
```bash
# Copier le template de configuration
cp .env.template .env

# Ã‰diter .env avec vos clÃ©s API
nano .env
```

## âš™ï¸ Configuration

### Variables d'environnement essentielles

#### Langfuse (Obligatoire)
```bash
LANGFUSE_PUBLIC_KEY=pk-lf-xxxxx
LANGFUSE_SECRET_KEY=sk-lf-xxxxx
LANGFUSE_BASE_URL=https://cloud.langfuse.com
```

#### Provider LLM pour Ragas
Choisir un provider et configurer les clÃ©s correspondantes :

**OpenAI** (RecommandÃ©)
```bash
RAGAS_LLM_PROVIDER=openai
RAGAS_MODEL_NAME=gpt-4.1-mini
OPENAI_API_KEY=sk-xxxxx
```

**Google Gemini**
```bash
RAGAS_LLM_PROVIDER=gemini
RAGAS_MODEL_NAME=gemini-2.5-flash
GOOGLE_API_KEY=AIzaSyxxxxx
```

**Anthropic Claude**
```bash
RAGAS_LLM_PROVIDER=claude
RAGAS_MODEL_NAME=claude-3-5-haiku-20241022
ANTHROPIC_API_KEY=sk-ant-api03-xxxxx
```

**Ollama (Local)**
```bash
RAGAS_LLM_PROVIDER=ollama
RAGAS_MODEL_NAME=llama2
RAGAS_OLLAMA_BASE_URL=http://localhost:11434
```

## ğŸ“Š PrÃ©paration des DonnÃ©es

### Format du CSV de rÃ©fÃ©rence
CrÃ©er un fichier `data/reference/reference_qa.csv` avec les colonnes suivantes :

```csv
question_id,question,reference_answer,expected_contexts,category,difficulty
Q001,"Question de test ?","RÃ©ponse attendue","Context1|||Context2|||Context3","CatÃ©gorie","Facile"
```

## ğŸ¯ Utilisation

### Ã‰valuation simple
```bash
source venv/bin/activate
python evaluate.py
```

### Avec options personnalisÃ©es
```bash
# Regarder les traces des 30 derniers jours
EVALUATION_TIMERANGE=30 python evaluate.py

# Utiliser un modÃ¨le diffÃ©rent
RAGAS_MODEL_NAME=gpt-4 python evaluate.py
```

## ğŸ“ˆ MÃ©triques Ragas

Le framework Ã©value les mÃ©triques suivantes :

- **Faithfulness** : La rÃ©ponse est-elle fidÃ¨le aux contextes rÃ©cupÃ©rÃ©s ?
- **Answer Relevancy** : La rÃ©ponse est-elle pertinente par rapport Ã  la question ?
- **Context Precision** : Les contextes rÃ©cupÃ©rÃ©s sont-ils pertinents ?
- **Context Recall** : Les contextes contiennent-ils toutes les informations nÃ©cessaires ?

## ğŸ“ RÃ©sultats

Les rÃ©sultats sont exportÃ©s dans :
- `data/results/smartrag_evaluation_results.csv` : RÃ©sultats tabulaires
- `data/results/smartrag_evaluation_results_detailed.json` : Analyse dÃ©taillÃ©e avec mÃ©tadonnÃ©es

## ğŸ” Workflow d'Ã‰valuation

1. **RÃ©cupÃ©ration des traces** : Connexion Ã  Langfuse pour rÃ©cupÃ©rer les traces SmartRAG
2. **Correspondance** : Matching des traces avec les questions de rÃ©fÃ©rence
3. **Ã‰valuation Ragas** : Calcul des mÃ©triques de qualitÃ©
4. **Export** : Sauvegarde des rÃ©sultats en CSV et JSON

## ğŸ› Troubleshooting

### ProblÃ¨me : "No module named 'pandas'"
```bash
pip install pandas langfuse ragas
```

### ProblÃ¨me : Scores Ã  0.0 avec Gemini/Claude
- VÃ©rifier que les contextes SmartRAG sont pertinents
- S'assurer que les documents sont bien indexÃ©s dans SmartRAG
- Essayer avec OpenAI qui est plus tolÃ©rant

### ProblÃ¨me : Timeout pendant l'Ã©valuation
Augmenter le timeout dans `.env` :
```bash
EVALUATION_BATCH_SIZE=2  # RÃ©duire la taille du batch
```

## ğŸ¤ Support des ModÃ¨les

| Provider | ModÃ¨les SupportÃ©s | Statut |
|----------|------------------|---------|
| OpenAI | gpt-4.1-mini, gpt-4o-mini, gpt-4, gpt-4-turbo | âœ… Stable |
| Gemini | gemini-2.5-flash, gemini-2.5-flash-lite, gemini-2.5-pro | âœ… Stable |
| Claude | claude-3-5-haiku-20241022, claude-3-5-sonnet-20241022, claude-3-5-opus | âœ… Stable |
| Ollama | mistral, qwen3, deepseek-r1 | âš ï¸ ExpÃ©rimental |

## ğŸ“š Documentation Additionnelle

- [Configuration Langfuse](https://langfuse.com/docs)
- [MÃ©triques Ragas](https://docs.ragas.io/en/latest/concepts/metrics/)
- [SmartRAG Documentation](Ã  venir)

## ğŸ”„ Mise Ã  jour

Pour mettre Ã  jour les dÃ©pendances :
```bash
pip install --upgrade langfuse ragas pandas
```

## ğŸ“„ License

Ce projet est sous licence MIT.